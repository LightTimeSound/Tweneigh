{
    "cells": [
     {
         "cell_type": "markdown",
         "source": [
             "# Gym Environments Tutorial"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Introduction to Gym Environments"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "Gym is an open-source toolkit for developing and comparing reinforcement learning algorithms. It provides a variety of environments for testing and training reinforcement learning agents. In this tutorial, we will go through the process of setting up a Gym environment, understanding the key concepts and components, training a simple agent, and evaluating its performance."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Setting up the Environment"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "To get started with Gym, you need to install the `gym` package. You can do this using `pip`:"
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "!pip install gym"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Creating a Gym Environment: CartPole-v0"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "For this tutorial, we will use the `CartPole-v0` environment, which is a classic control problem in reinforcement learning. In this environment, a pole is attached to a cart moving along a frictionless track. The objective is to keep the pole upright by applying forces to the cart. Let's create the environment and explore its components."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "import gym\n",
             "\n",
             "# Create the CartPole-v0 environment\n",
             "env = gym.make('CartPole-v0')\n",
             "\n",
             "# Reset the environment\n",
             "initial_state = env.reset()\n",
             "print('Initial state:', initial_state)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Understanding the Environment: Observation Space and Action Space"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "A Gym environment has two main components: the observation space and the action space. The observation space represents the state of the environment, while the action space represents the possible actions an agent can take in that environment. Let's explore these components for our CartPole-v0 environment."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Observation space and action space\n",
             "print('Observation space:', env.observation_space)\n",
             "print('Action space:', env.action_space)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Training a Simple Agent"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "Now that we have set up our environment and understand its components, let's train a simple agent using random actions. We will run this agent for a fixed number of episodes, and at each time step, we will choose a random action from the action space."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "import random\n",
             "\n",
             "num_episodes = 100\n",
             "episode_rewards = []\n",
             "\n",
             "for episode in range(num_episodes):\n",
             "    state = env.reset()\n",
             "    done = False\n",
             "    episode_reward = 0\n",
             "\n",
             "    while not done:\n",
             "        action = env.action_space.sample()  # Choose a random action\n",
             "        state, reward, done, _ = env.step(action)  # Perform the action and update the environment\n",
             "        episode_reward += reward\n",
             "\n",
             "    episode_rewards.append(episode_reward)\n",
             "\n",
             "# Calculate average reward across episodes\n",
             "average_reward = sum(episode_rewards) / num_episodes\n",
             "print('Average reward:', average_reward)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Evaluating the Agent's Performance"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "To evaluate the agent's performance, we can look at the average reward obtained across episodes. The higher the reward, the better the agent performed. Keep in mind that this simple agent is only taking random actions and not learning from its experiences. More advanced reinforcement learning algorithms can learn from their experiences and perform much better."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Saving and Loading a Trained Agent"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "When training more advanced reinforcement learning agents, it is important to save the trained agent's model so that it can be loaded and reused later. In this section, we will demonstrate how to save and load a simple agent's model using PyTorch. Although our example agent is not learning from its experiences, the process of saving and loading a model remains the same for more advanced agents."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "**Step 1**: Define the agent's model (for demonstration purposes, we will use a simple linear model)."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "import torch\n",
             "import torch.nn as nn\n",
             "\n",
             "# Define the agent's model\n",
             "class SimpleModel(nn.Module):\n",
             "    def __init__(self, input_size, output_size):\n",
             "        super(SimpleModel, self).__init__()\n",
             "        self.fc = nn.Linear(input_size, output_size)\n",
             "\n",
             "    def forward(self, x):\n",
             "        return self.fc(x)\n",
             "\n",
             "# Create an instance of the agent's model\n",
             "model = SimpleModel(env.observation_space.shape[0], env.action_space.n)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "**Step 2**: Save the agent's model to a file."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Save the model\n",
             "torch.save(model.state_dict(), 'simple_model.pth')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "**Step 3**: Load the agent's model from a file."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Load the model\n",
             "loaded_model = SimpleModel(env.observation_space.shape[0], env.action_space.n)\n",
             "loaded_model.load_state_dict(torch.load('simple_model.pth'))\n",
             "\n",
             "# Set the loaded model to evaluation mode\n",
             "loaded_model.eval()"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "# Practical Applications"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "Gym environments can be used to train reinforcement learning agents for a variety of tasks, including robotics, control, and optimization problems. By using more advanced reinforcement learning algorithms, such as Q-learning or policy gradients, agents can learn to perform complex tasks and optimize their actions based on their experiences. These trained agents can then be applied in practical situations, such as controlling robots, optimizing traffic signals, or managing resources in a data center."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Conclusion"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "In this tutorial, we covered the basics of Gym environments, including setting up an environment, understanding its components, training a simple agent, and evaluating its performance. We also demonstrated how to save and load a trained agent's model using PyTorch. With this foundation, you can explore more advanced reinforcement learning algorithms and apply them to various Gym environments for different tasks and applications."
         ],
         "metadata": {}
     }
 ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.11.1"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }
