{
    "cells": [
     {
         "cell_type": "markdown",
         "source": [
             "# Observation and Action Spaces in Gym"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "In this tutorial, we will learn about observation and action spaces in Gym, a popular Python library for reinforcement learning. We will cover the following topics:\n",
             "\n",
             "1. Understanding observation and action spaces\n",
             "2. Exploring Gym environments\n",
             "3. Working with different types of observation and action spaces\n",
             "4. Implementing a simple agent\n",
             "5. Evaluating the agent's performance"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Understanding Observation and Action Spaces"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "In reinforcement learning, an agent interacts with an environment to learn how to perform actions that maximize its cumulative reward. The environment provides the agent with observations, which represent the current state of the environment, and the agent takes actions based on these observations.\n",
             "\n",
             "**Observation space**: The set of all possible observations that an agent can encounter in an environment.\n",
             "\n",
             "**Action space**: The set of all possible actions that an agent can take in an environment.\n",
             "\n",
             "Gym provides a standardized interface for working with different environments and their respective observation and action spaces."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Exploring Gym Environments"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "First, let's install Gym and import it."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "!pip install gym\n",
             "import gym"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "Next, we will create an environment and explore its observation and action spaces. For this tutorial, we will use the 'MountainCar-v0' environment."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Create the environment\n",
             "env = gym.make('MountainCar-v0')\n",
             "\n",
             "# Print observation and action spaces\n",
             "print('Observation space:', env.observation_space)\n",
             "print('Action space:', env.action_space)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Working with Different Types of Observation and Action Spaces"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "Gym provides two main types of spaces:\n",
             "\n",
             "- `Discrete(n)`: A discrete space with `n` possible actions or observations.\n",
             "- `Box(shape)`: A continuous space with a given shape, representing a multidimensional array with lower and upper bounds.\n",
             "\n",
             "In the 'MountainCar-v0' environment, the observation space is continuous, while the action space is discrete. Let's explore their properties and sample some random actions and observations."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Sampling random actions and observations\n",
             "\n",
             "# Sample a random action\n",
             "action = env.action_space.sample()\n",
             "\n",
             "# Sample a random observation\n",
             "observation = env.observation_space.sample()\n",
             "\n",
             "print('Sampled action:', action)\n",
             "print('Sampled observation:', observation)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Implementing a Simple Agent"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "Now, let's implement a simple agent that takes random actions in the 'MountainCar-v0' environment. We will run the agent for 10 episodes and record the total reward obtained in each episode."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Implementing the random agent\n",
             "\n",
             "num_episodes = 10\n",
             "\n",
             "# Loop over episodes\n",
             "for episode in range(num_episodes):\n",
             "    observation = env.reset()\n",
             "    total_reward = 0\n",
             "\n",
             "    while True:\n",
             "        # Take a random action\n",
             "        action = env.action_space.sample()\n",
             "\n",
             "        # Perform the action and receive new observation, reward, and done flag\n",
             "        observation, reward, done, info = env.step(action)\n",
             "\n",
             "        # Update the total reward\n",
             "        total_reward += reward\n",
             "\n",
             "        if done:\n",
             "            break\n",
             "\n",
             "    print(f'Episode {episode + 1}: Total reward = {total_reward}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Evaluating the Agent's Performance"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "In this section, we will evaluate the performance of our random agent. We will calculate the average reward obtained over 10 episodes and discuss the results."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Evaluating the random agent\n",
             "\n",
             "num_episodes = 10\n",
             "total_rewards = []\n",
             "\n",
             "# Loop over episodes\n",
             "for episode in range(num_episodes):\n",
             "    observation = env.reset()\n",
             "    total_reward = 0\n",
             "\n",
             "    while True:\n",
             "        # Take a random action\n",
             "        action = env.action_space.sample()\n",
             "\n",
             "        # Perform the action and receive new observation, reward, and done flag\n",
             "        observation, reward, done, info = env.step(action)\n",
             "\n",
             "        # Update the total reward\n",
             "        total_reward += reward\n",
             "\n",
             "        if done:\n",
             "            break\n",
             "\n",
             "    total_rewards.append(total_reward)\n",
             "\n",
             "# Calculate average reward\n",
             "avg_reward = sum(total_rewards) / num_episodes\n",
             "\n",
             "# Print results\n",
             "print(f'Average reward over {num_episodes} episodes: {avg_reward}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "---\n",
             "**Conclusion:**\n",
             "- We learned about observation and action spaces in Gym and how to work with different types of spaces.\n",
             "- We explored the 'MountainCar-v0' environment and implemented a simple random agent.\n",
             "- We evaluated the agent's performance by calculating the average reward over multiple episodes."
         ],
         "metadata": {}
     }
 ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.11.1"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }
