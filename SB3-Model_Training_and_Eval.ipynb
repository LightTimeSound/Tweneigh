{
   "cells": [
    {
        "cell_type": "markdown",
        "source": [
            "# Model Training and Evaluation using stable_baselines3\n",
            "\n",
            "In this tutorial, we will learn how to train and evaluate reinforcement learning models using the `stable_baselines3` library, which is built on top of PyTorch. We will cover the following steps:\n",
            "\n",
            "1. Installing stable_baselines3\n",
            "2. Importing necessary libraries\n",
            "3. Creating a custom gym environment\n",
            "4. Training the model\n",
            "5. Evaluating the model\n",
            "6. Saving and loading the model\n",
            "7. Practical application\n",
            "8. Next steps"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Step 1: Installing stable_baselines3\n",
            "\n",
            "To install stable_baselines3, run the following command:"
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "!pip install stable-baselines3"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Step 2: Importing necessary libraries\n",
            "\n",
            "First, we need to import the necessary libraries for our tutorial."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "import gym\n",
            "import numpy as np\n",
            "from stable_baselines3 import PPO"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Step 3: Creating a custom gym environment\n",
            "\n",
            "For this tutorial, we will use the `CartPole-v1` environment from the OpenAI Gym. This environment provides a simple and classic reinforcement learning problem."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "env = gym.make('CartPole-v1')"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Step 4: Training the model\n",
            "\n",
            "Now, we will create a PPO agent and train it on the `CartPole-v1` environment. We'll use the default hyperparameters provided by stable_baselines3."        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "model = PPO('MlpPolicy', env, verbose=1)\n",
            "model.learn(total_timesteps=50000)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Step 5: Evaluating the model\n",
            "\n",
            "Once the model is trained, we can evaluate its performance by running it in the environment and observing the total reward it receives over multiple episodes."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "def evaluate(model, num_episodes=100):\n",
            "    episode_rewards = []\n",
            "    for _ in range(num_episodes):\n",
            "        obs = env.reset()\n",
            "        done = False\n",
            "        episode_reward = 0\n",
            "        while not done:\n",
            "            action, _ = model.predict(obs)\n",
            "            obs, reward, done, _ = env.step(action)\n",
            "            episode_reward += reward\n",
            "        episode_rewards.append(episode_reward)\n",
            "    return np.mean(episode_rewards), np.std(episode_rewards)\n",
            "\n",
            "mean_reward, std_reward = evaluate(model)\n",
            "print(f'Mean reward: {mean_reward}, Standard deviation: {std_reward}')"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Step 6: Saving and loading the model\n",
            "\n",
            "We can save the trained model to a file and load it later for reuse or further training."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "model.save('ppo_cartpole')\n",
            "loaded_model = PPO.load('ppo_cartpole')"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Step 7: Practical application\n",
            "\n",
            "Now that we have a trained model, we can use it for various practical applications, such as controlling a robotic arm, playing a game, or optimizing a system. In this example, we will visualize the trained model controlling the CartPole environment."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "from IPython import display\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "def visualize_agent(model, num_episodes=1):\n",
            "    for _ in range(num_episodes):\n",
            "        obs = env.reset()\n",
            "        done = False\n",
            "        while not done:\n",
            "            plt.imshow(env.render(mode='rgb_array'))\n",
            "            display.display(plt.gcf())\n",
            "            display.clear_output(wait=True)\n",
            "            action, _ = model.predict(obs)\n",
            "            obs, _, done, _ = env.step(action)\n",
            "        env.close()\n",
            "\n",
            "visualize_agent(loaded_model)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Step 8: Next steps\n",
            "\n",
            "Congratulations! You have successfully trained and evaluated a reinforcement learning model using stable_baselines3. To further your knowledge, consider exploring the following topics:\n",
            "\n",
            "- Experiment with other environments and algorithms in stable_baselines3\n",
            "- Learn how to create custom gym environments for your specific problems\n",
            "- Fine-tune the hyperparameters of the model to improve performance\n",
            "- Learn about other reinforcement learning libraries and frameworks"
        ],
        "metadata": {}
    }
],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
