{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "## Introduction to the Soft Actor-Critic (SAC) Algorithm\n",
           "Soft Actor-Critic (SAC) is an off-policy reinforcement learning algorithm that combines the strengths of both off-policy and on-policy methods. It is based on the maximum entropy framework which seeks to maximize both the expected return and the entropy of the policy. The increased exploration provided by the entropy term helps the algorithm perform well in a wide range of tasks. SAC is particularly effective in continuous control tasks with high-dimensional state and action spaces.\n",
           "\n",
           "In this tutorial, we will use the stable_baselines3 library, which is built on top of PyTorch, to implement the SAC algorithm. We will also use the gym library to set up the environment for training and evaluating our agent."
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "## Installing Necessary Libraries\n",
           "Before we begin, let's ensure we have the necessary libraries installed. We need to install stable_baselines3 and gym for this tutorial. You can install them using the following pip command:"
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "!pip install stable-baselines3 gym"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "## Setting up the Gym Environment\n",
           "We need to choose an environment to train our SAC agent. In this tutorial, we will use the 'Pendulum-v0' environment from the gym library. This environment consists of a pendulum with one joint that needs to be controlled to balance it in an upright position. The state space is continuous and has three dimensions, while the action space is also continuous and has one dimension."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "import gym\n",
           "\n",
           "env = gym.make('Pendulum-v0')"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "## Training the SAC Agent\n",
           "Now that we have set up the environment, we can proceed to train our SAC agent. We will use the `SAC` class from the stable_baselines3 library. First, we need to import the necessary classes and create an instance of the SAC agent. We also need to set the number of training steps for the agent."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "from stable_baselines3 import SAC\n",
           "from stable_baselines3.common.vec_env import DummyVecEnv\n",
           "\n",
           "env = DummyVecEnv([lambda: env])  # Vectorize the environment for stable_baselines3\n",
           "model = SAC('MlpPolicy', env, verbose=1)\n",
           "training_steps = 100000"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "With the agent initialized, we can now train it using the `learn()` method. This method takes the number of training steps as an argument and trains the agent accordingly. The training process may take some time depending on your system's performance."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "model.learn(total_timesteps=training_steps)"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "## Evaluating the Trained Agent\n",
           "After training the SAC agent, we can evaluate its performance by observing its behavior in the environment. We will run the agent for a certain number of episodes and render the environment to visualize the agent's actions."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "num_episodes = 5\n",
           "evaluation_env = gym.make('Pendulum-v0')\n",
           "\n",
           "for episode in range(num_episodes):\n",
           "    obs = evaluation_env.reset()\n",
           "    done = False\n",
           "    episode_reward = 0\n",
           "\n",
           "    while not done:\n",
           "        action, _ = model.predict(obs, deterministic=True)\n",
           "        obs, reward, done, info = evaluation_env.step(action)\n",
           "        episode_reward += reward\n",
           "        evaluation_env.render()\n",
           "\n",
           "    print(f'Episode {episode + 1}: Reward = {episode_reward}')\n",
           "\n",
           "evaluation_env.close()"
         ]
       }
   ]
   ,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
