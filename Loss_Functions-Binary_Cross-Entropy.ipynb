{
   "cells": [
    {
        "cell_type": "markdown",
        "source": [
            "# Binary Cross-Entropy Loss\n",
            "\n",
            "In this tutorial, we will learn about Binary Cross-Entropy Loss, a widely used loss function for binary classification tasks. We'll cover the following topics:\n",
            "\n",
            "1. Key concepts of Binary Cross-Entropy Loss\n",
            "2. Implementing Binary Cross-Entropy Loss in PyTorch\n",
            "3. Understanding and adjusting the parameters\n",
            "4. Training a simple binary classification model\n",
            "5. Saving and loading the trained model\n",
            "6. Evaluating and interpreting the results\n",
            "7. Practical application of Binary Cross-Entropy Loss\n"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Key Concepts of Binary Cross-Entropy Loss\n",
            "\n",
            "Binary Cross-Entropy Loss, also known as Log Loss or Sigmoid Cross-Entropy Loss, is used in binary classification tasks where the output is expected to be either 0 or 1. It measures the performance of a classification model whose output is a probability value between 0 and 1. The loss increases as the predicted probability diverges from the actual label.\n",
            "\n",
            "The formula for Binary Cross-Entropy Loss is:\n",
            "\n",
            "$$L(y, \\hat{y}) = - (y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}))$$\n",
            "\n",
            "where $y$ is the true label (0 or 1) and $\\hat{y}$ is the predicted probability.\n"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Implementing Binary Cross-Entropy Loss in PyTorch\n",
            "\n",
            "In PyTorch, we can use the `BCELoss` class to compute the Binary Cross-Entropy Loss. Let's import the necessary libraries and create a simple example."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "import torch\n",
            "import torch.nn as nn\n",
            "\n",
            "# True labels\n",
            "y = torch.tensor([1, 0, 1, 0], dtype=torch.float32)\n",
            "\n",
            "# Predicted probabilities\n",
            "y_pred = torch.tensor([0.9, 0.1, 0.8, 0.2], dtype=torch.float32)\n",
            "\n",
            "# Binary Cross-Entropy Loss\n",
            "criterion = nn.BCELoss()\n",
            "loss = criterion(y_pred, y)\n",
            "print('Binary Cross-Entropy Loss:', loss.item())"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Understanding and Adjusting the Parameters\n",
            "\n",
            "The `BCELoss` class has an optional parameter `reduction` which specifies the reduction to apply to the output. It has three possible values:\n",
            "\n",
            "1. 'mean' (default): The sum of the output is divided by the number of elements.\n",
            "2. 'sum': The output will be summed.\n",
            "3. 'none': No reduction will be applied, and the loss will be returned for each input element.\n",
            "\n",
            "Let's see an example of how changing the `reduction` parameter affects the output."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "# Binary Cross-Entropy Loss with 'sum' reduction\n",
            "criterion_sum = nn.BCELoss(reduction='sum')\n",
            "loss_sum = criterion_sum(y_pred, y)\n",
            "print('Binary Cross-Entropy Loss (sum reduction):', loss_sum.item())\n",
            "\n",
            "# Binary Cross-Entropy Loss with 'none' reduction\n",
            "criterion_none = nn.BCELoss(reduction='none')\n",
            "loss_none = criterion_none(y_pred, y)\n",
            "print('Binary Cross-Entropy Loss (none reduction):', loss_none)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Training a Simple Binary Classification Model\n",
            "\n",
            "Now, let's create a simple binary classification model using PyTorch and train it using Binary Cross-Entropy Loss. We'll use a synthetic dataset for this example."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "import torch.optim as optim\n",
            "\n",
            "# Synthetic dataset\n",
            "X = torch.randn(100, 1)\n",
            "y = (X > 0).float().squeeze()\n",
            "\n",
            "# Simple binary classification model\n",
            "class SimpleClassifier(nn.Module):\n",
            "    def __init__(self):\n",
            "        super(SimpleClassifier, self).__init__()\n",
            "        self.linear = nn.Linear(1, 1)\n",
            "        self.sigmoid = nn.Sigmoid()\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.linear(x)\n",
            "        x = self.sigmoid(x)\n",
            "        return x\n",
            "\n",
            "model = SimpleClassifier()\n",
            "criterion = nn.BCELoss()\n",
            "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
            "\n",
            "# Training loop\n",
            "for epoch in range(100):\n",
            "    optimizer.zero_grad()\n",
            "    y_pred = model(X).squeeze()\n",
            "    loss = criterion(y_pred, y)\n",
            "    loss.backward()\n",
            "    optimizer.step()\n",
            "    if (epoch + 1) % 10 == 0:\n",
            "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, 100, loss.item()))"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Saving and Loading the Trained Model\n",
            "\n",
            "Once the model is trained, we can save its parameters and load them later for reuse. Here's how to do that in PyTorch."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "# Save the model parameters\n",
            "torch.save(model.state_dict(), 'simple_classifier.pth')\n",
            "\n",
            "# Load the model parameters\n",
            "loaded_model = SimpleClassifier()\n",
            "loaded_model.load_state_dict(torch.load('simple_classifier.pth'))"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Evaluating and Interpreting the Results\n",
            "\n",
            "To evaluate the model, we can compute the Binary Cross-Entropy Loss on a validation set. We can also compute other metrics like accuracy, precision, recall, and F1-score to better understand the model's performance."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "# Synthetic validation dataset\n",
            "X_val = torch.randn(20, 1)\n",
            "y_val = (X_val > 0).float().squeeze()\n",
            "\n",
            "# Compute the validation loss\n",
            "y_val_pred = loaded_model(X_val).squeeze()\n",
            "val_loss = criterion(y_val_pred, y_val)\n",
            "print('Validation Loss:', val_loss.item())\n",
            "\n",
            "# Compute accuracy\n",
            "y_val_pred_class = (y_val_pred > 0.5).float()\n",
            "accuracy = (y_val_pred_class == y_val).float().mean()\n",
            "print('Accuracy:', accuracy.item())"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Practical Application of Binary Cross-Entropy Loss\n",
            "\n",
            "Binary Cross-Entropy Loss is widely used in various real-world applications, such as:\n",
            "\n",
            "1. Medical diagnosis: Classifying whether a patient has a specific disease or not.\n",
            "2. Spam detection: Identifying whether an email is spam or not.\n",
            "3. Sentiment analysis: Predicting whether a given text has a positive or negative sentiment.\n",
            "4. Fraud detection: Detecting whether a transaction is fraudulent or not.\n",
            "\n",
            "These are just a few examples, and there are many other applications where Binary Cross-Entropy Loss can be used effectively."   
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "# Combining Binary Cross-Entropy Loss with other Loss Functions\n",
            "\n",
            "In some cases, you might want to combine Binary Cross-Entropy Loss with other loss functions to optimize your model's performance. For example, you may want to optimize both classification and localization in an object detection task. Let's see an example of how to combine Binary Cross-Entropy Loss with Mean Squared Error (MSE) Loss."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "# Synthetic dataset\n",
            "X = torch.randn(100, 1)\n",
            "y_classification = (X > 0).float().squeeze()\n",
            "y_regression = X * 2\n",
            "\n",
            "# Simple combined model\n",
            "class CombinedModel(nn.Module):\n",
            "    def __init__(self):\n",
            "        super(CombinedModel, self).__init__()\n",
            "        self.linear1 = nn.Linear(1, 1)\n",
            "        self.linear2 = nn.Linear(1, 1)\n",
            "        self.sigmoid = nn.Sigmoid()\n",
            "\n",
            "    def forward(self, x):\n",
            "        classification = self.sigmoid(self.linear1(x))\n",
            "        regression = self.linear2(x)\n",
            "        return classification, regression\n",
            "\n",
            "model = CombinedModel()\n",
            "criterion_bce = nn.BCELoss()\n",
            "criterion_mse = nn.MSELoss()\n",
            "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
            "\n",
            "# Training loop\n",
            "for epoch in range(100):\n",
            "    optimizer.zero_grad()\n",
            "    y_pred_classification, y_pred_regression = model(X)\n",
            "    loss_classification = criterion_bce(y_pred_classification.squeeze(), y_classification)\n",
            "    loss_regression = criterion_mse(y_pred_regression, y_regression)\n",
            "    loss = loss_classification + loss_regression\n",
            "    loss.backward()\n",
            "    optimizer.step()\n",
            "    if (epoch + 1) % 10 == 0:\n",
            "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, 100, loss.item()))"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "In the example above, we created a simple model that outputs both a classification and a regression prediction. We used Binary Cross-Entropy Loss for the classification task and Mean Squared Error Loss for the regression task. During training, we combined both losses and optimized the model using the combined loss.\n",
            "\n",
            "This approach can be applied to more complex models and additional loss functions as needed, depending on the specific problem you're trying to solve."
        ],
        "metadata": {}
    }
],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
