{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Neural Networks in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neural networks are the foundation of deep learning, and PyTorch provides a flexible and easy-to-use interface for defining, training, and using them. The primary components for working with neural networks in PyTorch are the `torch.nn` module and its submodules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are the key concepts and components to understand when working with neural networks in PyTorch:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. `nn.Module`: The base class for all neural network modules in PyTorch. It provides a clean and modular way to define custom network architectures. To create a custom network, you need to subclass `nn.Module`, define the layers and operations in the constructor, and implement the `forward()` method to specify the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class CustomNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomNetwork, self).__init__()\n",
        "        # Define layers and operations here\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass here\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Layers and operations: PyTorch provides a variety of predefined layers and operations for building neural networks, available as classes and functions in the `torch.nn` and `torch.nn.functional` modules. Some common layers include:\n",
        "- `nn.Linear`: Fully connected (dense) layer.\n",
        "- `nn.Conv2d`, `nn.Conv1d`, `nn.Conv3d`: Convolutional layers.\n",
        "- `nn.MaxPool2d`, `nn.AvgPool2d`: Pooling layers.\n",
        "- `nn.BatchNorm2d`, `nn.BatchNorm1d`: Batch normalization layers.\n",
        "- `nn.Dropout`, `nn.Dropout2d`: Dropout layers for regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example of using predefined layers\n",
        "linear = nn.Linear(10, 5)\n",
        "conv2d = nn.Conv2d(3, 64, kernel_size=5)\n",
        "maxpool2d = nn.MaxPool2d(2)\n",
        "dropout = nn.Dropout(p=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Activation functions: PyTorch provides common activation functions as classes in the `torch.nn` module and as functions in the `torch.nn.functional` module. Some common activation functions include:\n",
        "- `nn.ReLU`, `nn.functional.relu`\n",
        "- `nn.Sigmoid`, `nn.functional.sigmoid`\n",
        "- `nn.Tanh`, `nn.functional.tanh`\n",
        "- `nn.Softmax`, `nn.functional.softmax`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example of using activation functions\n",
        "relu = nn.ReLU()\n",
        "sigmoid = nn.Sigmoid()\n",
        "tanh = nn.Tanh()\n",
        "softmax = nn.Softmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Loss functions: PyTorch provides a variety of loss functions for different tasks in the `torch.nn` module, including:\n",
        "- `nn.MSELoss`: Mean squared error loss for regression tasks.\n",
        "- `nn.CrossEntropyLoss`: Cross-entropy loss for classification tasks (includes softmax activation).\n",
        "- `nn.BCELoss`, `nn.BCEWithLogitsLoss`: Binary cross-entropy loss for binary classification tasks.\n",
        "- `nn.L1Loss`: L1 loss for regression tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example of using loss functions\n",
        "mse_loss = nn.MSELoss()\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "bce_loss = nn.BCELoss()\n",
        "l1_loss = nn.L1Loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Training loop: In order to train a neural network in PyTorch, you'll typically follow these steps in a loop:\n",
        "- Perform a forward pass through the network to compute predictions.\n",
        "- Calculate the loss using a suitable loss function.\n",
        "- Call `backward()` on the loss tensor to compute gradients.\n",
        "- Update the model's parameters using an optimizer.\n",
        "- Zero the gradients to prevent accumulation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simplified example of a training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        \n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's see a simple example of defining a custom neural network in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
