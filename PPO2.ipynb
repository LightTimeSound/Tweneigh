{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "## Proximal Policy Optimization (PPO)\n",
          "\n",
          "Proximal Policy Optimization (PPO) is a popular reinforcement learning algorithm that works well in many environments. PPO is an on-policy algorithm that combines the benefits of trust region policy optimization (TRPO) with the simplicity of the first-order optimization. In this tutorial, we will implement the PPO algorithm using stable_baselines3, gym, and PyTorch."
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Installation\n",
          "\n",
          "Before we begin, let's install the required libraries:"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "!pip install stable-baselines3[extra]\n",
          "!pip install gym"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Import Libraries\n",
          "\n",
          "Now, let's import the necessary libraries:"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "import gym\n",
          "from stable_baselines3 import PPO\n",
          "from stable_baselines3.common.vec_env import DummyVecEnv\n",
          "from stable_baselines3.common.evaluation import evaluate_policy"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Create and Wrap the Environment\n",
          "\n",
          "We will use the CartPole environment from the gym library as an example. First, create the environment and wrap it using `DummyVecEnv`:"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "env = gym.make('CartPole-v1')\n",
          "env = DummyVecEnv([lambda: env])"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Instantiate and Train the PPO Model\n",
          "\n",
          "Next, we'll instantiate the PPO model with the environment and train it. You can adjust the hyperparameters as needed:"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "model = PPO('MlpPolicy', env, verbose=1)\n",
          "model.learn(total_timesteps=10000)"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Evaluate the Trained Model\n",
          "\n",
          "Once the model is trained, we can evaluate its performance using the `evaluate_policy` function:"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
          "print(f'Mean reward: {mean_reward} +/- {std_reward}')"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Save and Load the Model\n",
          "\n",
          "You can save the trained model to a file and load it later for further use:"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "model.save('ppo_cartpole')\n",
          "loaded_model = PPO.load('ppo_cartpole')"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Test the Loaded Model\n",
          "\n",
          "Finally, let's test the loaded model by running a single episode in the CartPole environment:"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "obs = env.reset()\n",
          "done = False\n",
          "while not done:\n",
          "    action, _ = loaded_model.predict(obs, deterministic=True)\n",
          "    obs, reward, done, info = env.step(action)\n",
          "    env.render()\n",
          "env.close()"
         ]
        }
   ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
