{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# PyTorch Autograd Tutorial\n",
       "\n",
       "Autograd is the automatic differentiation system in PyTorch that simplifies the computation of gradients for backpropagation during the training of neural networks. It tracks the operations performed on tensors to build a dynamic computational graph, which is then used to compute gradients with respect to the model's parameters.\n",
       "\n",
       "Here are some key concepts related to Autograd:"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Computational Graph\n",
       "A directed graph that represents the sequence of operations performed on tensors during a forward pass. Each node in the graph corresponds to an operation, and the edges represent the flow of tensor data between the operations."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. requires_grad\n",
       "A boolean attribute of tensors that indicates whether a tensor should track gradients for automatic differentiation. By default, it is set to False for input tensors and True for model parameters (e.g., weights and biases in a neural network)."
      ]
     },
     {
        "cell_type": "code",
        "metadata": {},
        "source": [
         "import torch\n",
         "\n",
         "x = torch.tensor([2.0], requires_grad=True)\n",
         "y = torch.tensor([3.0], requires_grad=True)\n",
         "print(f'x requires_grad: {x.requires_grad}')\n",
         "print(f'y requires_grad: {y.requires_grad}')"
        ],
        "execution_count": null,
        "outputs": []
      },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. backward()\n",
       "A method available on scalar tensors that computes gradients for all tensors in the computational graph with requires_grad=True. Typically, this method is called on the scalar output of a loss function after the forward pass. It accumulates gradients in the .grad attribute of the tensors involved in the computation."
      ]
     },
     {
        "cell_type": "code",
        "metadata": {},
        "source": [
         "z = (x ** 2) * y\n",
         "z.backward()\n",
         "print(f'dz/dx: {x.grad}')\n",
         "print(f'dz/dy: {y.grad}')"
        ],
        "execution_count": null,
        "outputs": []
      },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Gradient accumulation\n",
       "When calling the backward() method multiple times without zeroing gradients, they will accumulate. This is useful for certain optimization techniques but can lead to unexpected behavior if not handled properly. Use the .zero_grad() method of an optimizer or manually set the .grad attribute to zero for tensors to prevent this accumulation."
      ]
     },
     {
        "cell_type": "code",
        "metadata": {},
        "source": [
         "z.backward(retain_graph=True)\n",
         "print(f'Accumulated dz/dx: {x.grad}')\n",
         "print(f'Accumulated dz/dy: {y.grad}')\n",
         "x.grad.zero_()\n",
         "y.grad.zero_()"
        ],
        "execution_count": null,
        "outputs": []
      },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. .detach()\n",
       "A method that creates a new tensor without gradient tracking, sharing the same data as the original tensor. This is useful when you want to use a tensor's data for computations that should not be part of the computational graph."
      ]
     },
     {
        "cell_type": "code",
        "metadata": {},
        "source": [
         "detached_x = x.detach()\n",
         "print(f'detached_x requires_grad: {detached_x.requires_grad}')"
        ],
        "execution_count": null,
        "outputs": []
      },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. torch.no_grad()\n",
       "A context manager that temporarily disables gradient tracking for all tensors. This is useful when evaluating a model, as gradients are not needed and disabling tracking can improve performance and save memory."
      ]
     },
     {
        "cell_type": "code",
        "metadata": {},
        "source": [
         "with torch.no_grad():\n",
         "    y = x * 2\n",
         "print(f'y requires_grad: {y.requires_grad}')"
        ],
        "execution_count": null,
        "outputs": []
      },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "Now, let's see an example of using Autograd to compute gradients for a simple function:"
      ]
     },
     {
        "cell_type": "code",
        "metadata": {},
        "source": [
         "# Let's use Autograd to compute gradients for a simple function\n",
         "x = torch.tensor([3.0], requires_grad=True)\n",
         "y = x ** 2\n",
         "y.backward()\n",
         "print(f'dy/dx: {x.grad}')"
        ],
        "execution_count": null,
        "outputs": []
      },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "By understanding these concepts, you'll be able to leverage PyTorch's Autograd system to simplify gradient computations for your machine learning models."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }
   
