{
   "cells": [
   {
      "cell_type": "markdown",
      "id": "1304c8d3",
      "metadata": {},
      "source": [
      "## Overview of stable_baselines3\n",
      "\n",
      "Stable_baselines3 is a set of high-level reinforcement learning (RL) libraries that build on top of PyTorch. It provides an easy-to-use interface for implementing and training various RL algorithms, such as Proximal Policy Optimization (PPO), Deep Q-Networks (DQN), and Soft Actor-Critic (SAC). It is compatible with OpenAI's gym library, allowing you to test your RL algorithms on a wide range of environments.\n",
      "\n",
      "### Common Uses\n",
      "\n",
      "1. Implementing RL algorithms: Stable_baselines3 provides a simple interface for implementing various RL algorithms, such as PPO, DQN, and SAC.\n",
      "2. Training and evaluating RL agents: You can easily train and evaluate your RL agents using stable_baselines3's built-in functions.\n",
      "3. Customizing RL algorithms: Stable_baselines3 allows you to create custom feature extractors, actor-critic policies, and neural networks to fine-tune your RL agent's performance.\n",
      "4. Visualizing training progress: Stable_baselines3 integrates with TensorBoard for easy monitoring and visualization of your agent's training progress.\n",
      "\n",
      "Now let's move on to discussing CustomFeatureExtractors, CustomActorCriticPolicy, and implementing a neural network to learn the weights of feature columns."
      ]
   },
   {
      "cell_type": "code",
      "execution_count": null,
      "id": "badf7e7a",
      "metadata": {},
      "outputs": [],
      "source": [
      "# Installation of stable_baselines3\n",
      "!pip install stable-baselines3"
      ]
   },
   {
      "cell_type": "markdown",
      "id": "1304c8d3",
      "metadata": {},
      "source": [
      "## CustomFeatureExtractors\n",
      "\n",
      "A feature extractor is a neural network that extracts relevant features from raw input data. In stable_baselines3, you can create a custom feature extractor by subclassing `BaseFeaturesExtractor` and implementing the `forward` method.\n",
      "\n",
      "## CustomActorCriticPolicy\n",
      "\n",
      "A custom actor-critic policy allows you to define your own neural network architecture for both the actor and critic networks. To create a custom actor-critic policy, subclass `ActorCriticPolicy` and implement the `_build_mlp_extractor` and `_build_net` methods.\n",
      "\n",
      "## Implementing a Neural Network\n",
      "\n",
      "In this section, we'll implement a simple neural network to learn the weights of feature columns using PyTorch. Let's start by importing the necessary libraries."
      ]
   },

   {
      "cell_type": "code",
      "execution_count": null,
      "id": "badf7e7a",
      "metadata": {},
      "outputs": [],
      "source": [
      "import torch\n",
      "import torch.nn as nn\n",
      "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
      "from stable_baselines3 import PPO\n",
      "from stable_baselines3.ppo import MlpPolicy"
      ]
   },
   {
      "cell_type": "markdown",
      "id": "1304c8d3",
      "metadata": {},
      "source": [
      "### Creating a CustomFeatureExtractor\n",
      "\n",
      "Now let's create a custom feature extractor that extracts features from raw input data:"
      ]
   },

   {
      "cell_type": "code",
      "execution_count": null,
      "id": "badf7e7a",
      "metadata": {},
      "outputs": [],
      "source": [
      "class CustomFeatureExtractor(BaseFeaturesExtractor):\n",
      "    def __init__(self, observation_space: gym.Space, features_dim: int = 128):\n",
      "        super(CustomFeatureExtractor, self).__init__(observation_space, features_dim)\n",
      "        self.net = nn.Sequential(nn.Linear(observation_space.shape[0], 64),\n",
      "                                 nn.ReLU(),\n",
      "                                 nn.Linear(64, features_dim))\n",
      "\n",
      "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
      "        return self.net(observations)"
      ]
   },

   {
      "cell_type": "markdown",
      "id": "1304c8d3",
      "metadata": {},
      "source": [
      "### Implementing a CustomActorCriticPolicy\n",
      "\n",
      "Now let's create a custom actor-critic policy that uses our custom feature extractor:"
      ]
   },

   {
      "cell_type": "code",
      "execution_count": null,
      "id": "badf7e7a",
      "metadata": {},
      "outputs": [],
      "source": [
      "class CustomActorCriticPolicy(MlpPolicy):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super(CustomActorCriticPolicy, self).__init__(*args, **kwargs)\n",
      "\n",
      "    def _build_mlp_extractor(self) -> None:\n",
      "        self.mlp_extractor = CustomFeatureExtractor(self.observation_space)"
      ]
   },

   {
      "cell_type": "markdown",
      "id": "1304c8d3",
      "metadata": {},
      "source": [
      "Now you can use the custom feature extractor and actor-critic policy with the PPO algorithm in stable_baselines3. You can train the agent on a gym environment and evaluate its performance.\n",
      "\n",
      "This concludes the tutorial on CustomFeatureExtractors, CustomActorCriticPolicy, and implementing a neural network to learn the weights of feature columns."
      ]
   },

   {
      "cell_type": "code",
      "execution_count": null,
      "id": "badf7e7a",
      "metadata": {},
      "outputs": [],
      "source": [
      "# Example usage\n",
      "import gym\n",
      "\n",
      "env = gym.make('CartPole-v1')\n",
      "model = PPO(CustomActorCriticPolicy, env, verbose=1)\n",
      "model.learn(total_timesteps=10000)\n",
      "model.save('ppo_custom_policy')\n",
      "\n",
      "del model\n",
      "model = PPO.load('ppo_custom_policy', env=env)\n",
      "\n",
      "# Test the trained agent\n",
      "obs = env.reset()\n",
      "for _ in range(1000):\n",
      "    action, _state = model.predict(obs, deterministic=True)\n",
      "    obs, reward, done, info = env.step(action)\n",
      "    env.render()\n",
      "    if done:\n",
      "        obs = env.reset()\n",
      "\n",
      "env.close()"
      ]
   }
],
"metadata": {
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.11.1"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }
