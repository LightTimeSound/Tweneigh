{
    "cells": [
     {
         "cell_type": "markdown",
         "source": [
             "# Gym Reward Functions \n\nIn this tutorial, we will learn about Gym reward functions and how to use them within the context of PyTorch. We will be using the OpenAI Gym library to create a simple environment and implement a reward function to train an agent.\n\n## Prerequisites\n\nBefore we begin, make sure you have the following installed:\n\n- Python 3.6 or later\n- PyTorch\n- OpenAI Gym\n\nYou can install PyTorch and OpenAI Gym using pip:\n\n```\npip install torch\npip install gym\n```\n"
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Import the necessary libraries\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nprint('Libraries imported!')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Step 1: Create a Gym Environment\n\nFirst, let's create a simple Gym environment. We will be using the 'CartPole-v0' environment for this tutorial. The goal of this environment is to balance a pole on a cart by applying forces to the cart."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Create the Gym environment\n\nenv = gym.make('CartPole-v0')\nprint(env)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Step 2: Define the Neural Network\n\nNext, we will define a simple feedforward neural network using PyTorch. This network will take the state of the environment as input and output the action probabilities."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Define the neural network\n\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.softmax(self.fc2(x), dim=-1)\n        return x\n\ninput_size = env.observation_space.shape[0]\nhidden_size = 64\noutput_size = env.action_space.n\nmodel = Net(input_size, hidden_size, output_size)\nprint(model)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Step 3: Understand and Implement Different Reward Functions\n\nIn this step, we will dive deeper into reward functions and implement three different reward functions for the CartPole environment. The choice of reward function can significantly impact the agent's learning and performance.\n\n### Reward Function 1: Basic Reward\n\nThis is the simplest reward function, where the agent receives a reward of +1 for every time step the pole is balanced, and -1 if the episode ends."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "def basic_reward(state, action, next_state, done):\n    if done:\n        return -1\n    else:\n        return 1\n\nprint('Basic reward function defined!')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### Reward Function 2: Pole Angle\n\nIn this reward function, the agent receives a reward based on the absolute angle of the pole. The closer the pole is to being upright, the higher the reward."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "def angle_reward(state, action, next_state, done):\n    pole_angle = abs(next_state[2])\n    if done:\n        return -1\n    else:\n        return 1 - pole_angle / (2 * 3.14159265)\n\nprint('Pole angle reward function defined!')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### Reward Function 3: Pole Angle and Cart Position\n\nIn this reward function, the agent receives a reward based on both the absolute angle of the pole and the cart's position. The closer the pole is to being upright and the closer the cart is to the center, the higher the reward."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "def angle_position_reward(state, action, next_state, done):\n    pole_angle = abs(next_state[2])\n    cart_position = abs(next_state[0])\n    if done:\n        return -1\n    else:\n        return 1 - (pole_angle / (2 * 3.14159265) + cart_position / 2.4) / 2\n\nprint('Pole angle and cart position reward function defined!')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "Now, you can choose one of these reward functions and replace `get_reward` in Step 4 with the chosen reward function. This will allow you to see how different reward functions impact the agent's learning and performance."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Step 4: Train the Agent\n\nNow, we will train the agent using the neural network, reward function, and Gym environment. We will use the REINFORCE algorithm for training."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Training parameters\nnum_episodes = 1000\nlearning_rate = 0.01\n\ngamma = 0.99\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train the agent\nfor episode in range(num_episodes):\n    state = env.reset()\n    rewards = []\n    log_probs = []\n    done = False\n    \n    while not done:\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        action_probs = model(state_tensor)\n        action = torch.multinomial(action_probs, 1).item()\n        log_prob = torch.log(action_probs[0, action])\n        \n        next_state, _, done, _ = env.step(action)\n        reward = get_reward(state, action, next_state, done)\n        \n        rewards.append(reward)\n        log_probs.append(log_prob)\n\n        state = next_state\n    \n    # Update the model\n    R = 0\n    policy_loss = []\n    for r, log_prob in zip(reversed(rewards), reversed(log_probs)):\n        R = r + gamma * R\n        policy_loss.append(-log_prob * R)\n    policy_loss = torch.cat(policy_loss).sum()\n    \n    optimizer.zero_grad()\n    policy_loss.backward()\n    optimizer.step()\n    \n    if episode % 50 == 0:\n        print(f'Episode {episode}, Loss: {policy_loss.item()}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Step 5: Evaluate the Trained Agent\n\nAfter training, we can evaluate the agent by running it in the environment and observing its performance." 
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Evaluate the agent\nnum_eval_episodes = 10\n\nfor episode in range(num_eval_episodes):\n    state = env.reset()\n    done = False\n    total_reward = 0\n    \n    while not done:\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        action_probs = model(state_tensor)\n        action = torch.argmax(action_probs, 1).item()\n        \n        next_state, _, done, _ = env.step(action)\n        reward = get_reward(state, action, next_state, done)\n    total_reward += reward\n\n        state = next_state\n    \n    print(f'Episode {episode + 1}, Total Reward: {total_reward}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Next Steps\n\nIn this tutorial, we learned about Gym reward functions and their implementation in the context of PyTorch. We used the OpenAI Gym library to create a simple environment, defined a neural network using PyTorch, and trained an agent using the REINFORCE algorithm.\n\nNext, you can explore more complex environments and algorithms for reinforcement learning, such as Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Actor-Critic Methods."   
         ],
         "metadata": {}
     }
 ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.11.1"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }
