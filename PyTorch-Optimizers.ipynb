{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Optimizers in PyTorch are responsible for updating model parameters during training, based on the computed gradients from the backpropagation process. The torch.optim module provides a variety of optimization algorithms that can be used to train neural networks."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Here are some key concepts and common optimizers to understand when working with PyTorch:\n\n1. torch.optim.Optimizer: The base class for all optimizers in PyTorch. You typically don't need to interact with this class directly, but rather use one of the predefined optimizer classes.\n\n2. Common optimizers:\n\n• torch.optim.SGD: Stochastic Gradient Descent (SGD), a widely used optimization algorithm. It can also include momentum and weight decay for improved performance.\n\n• torch.optim.Adam: Adaptive Moment Estimation (Adam), a popular optimization algorithm that adapts the learning rate for each parameter based on the first and second moments of the gradients.\n\n• torch.optim.RMSprop: Root Mean Square Propagation (RMSprop), another popular optimization algorithm that adapts the learning rate for each parameter based on the moving average of the squared gradients.\n\n• torch.optim.Adagrad: Adaptive Gradient Algorithm (Adagrad), an optimization algorithm that adapts the learning rate for each parameter based on the sum of squared gradients."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch.optim as optim\n\n# Example of creating an optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "3. Creating an optimizer: To create an optimizer, you need to pass the model's parameters (obtained using the parameters() method of an nn.Module instance) and the learning rate (or other hyperparameters specific to the optimizer)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch.optim as optim\n\n# Assuming 'net' is your model\noptimizer = optim.Adam(net.parameters(), lr=0.001)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "4. Updating model parameters: In the training loop, after computing gradients using the backward() method, you need to update the model's parameters using the optimizer. This is done by calling the step() method of the optimizer instance."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Assuming 'loss' is the calculated loss\nloss.backward()\n\n# Update model parameters\noptimizer.step()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "5. Zeroing gradients: After updating the model's parameters, it's important to zero the gradients to prevent accumulation. This is done by calling the zero_grad() method of the optimizer instance. This should be done at the beginning of the training loop or right after the step() method."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "optimizer.zero_grad()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Here is an example of a typical training loop that incorporates these steps:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Assume net is an instance of a custom neural network, and data_loader is a DataLoader instance\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n\nfor epoch in range(epochs):\n    for inputs, targets in data_loader:\n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = net(inputs)\n        \n        # Calculate loss\n        loss = loss_function(outputs, targets)\n        \n        # Backward pass (compute gradients)\n        loss.backward()\n        \n        # Update model parameters\n        optimizer.step()"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
