{
   "cells": [
    {
        "cell_type": "markdown",
        "source": [
            "# Negative Log Likelihood (NLL) Loss in PyTorch"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Explanation of Key Concepts"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "Negative Log Likelihood (NLL) Loss is a commonly used loss function for classification problems with multiple classes. It's particularly useful when working with probability distributions as outputs from your model, such as when using the softmax activation function in the final layer."
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Contextualize the Topic"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "In this tutorial, we'll use the Iris dataset to demonstrate how to apply NLL loss. The Iris dataset is a popular dataset containing 150 samples of iris flowers with four features (sepal length, sepal width, petal length, petal width) and three classes (Setosa, Versicolor, Virginica)."
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Explain Parameters and Settings"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "In PyTorch, you can use the `nn.NLLLoss` class to compute the NLL loss. It has two main parameters:"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "- `reduction`: Specifies the reduction method applied to the output. It can be 'mean' (default), 'sum', or 'none'. The 'mean' calculates the mean of the loss values, 'sum' calculates their sum, and 'none' returns the individual losses without any reduction."
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "- `weight`: An optional tensor of weights to be used for each class, useful when dealing with imbalanced datasets. By default, it's set to `None`. If provided, the loss will be multiplied by the weights for each class."
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Training Process"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "First, let's import the necessary libraries, load the Iris dataset, and split it into training and testing sets."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "import torch\nfrom torch import nn\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Standardize the data\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert to tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.long)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "Now, let's create a simple feed-forward neural network with a softmax activation function in the final layer."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "class IrisClassifier(nn.Module):\n    def __init__(self):\n        super(IrisClassifier, self).__init__()\n        self.fc1 = nn.Linear(4, 16)\n    self.fc2 = nn.Linear(16, 8)\n        self.fc3 = nn.Linear(8, 3)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.log_softmax(self.fc3(x), dim=1)\n        return x\n\nmodel = IrisClassifier()"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "Now, let's define the NLL loss and the optimizer."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "criterion = nn.NLLLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "Let's train the model for 100 epochs and print the training loss at each epoch."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "num_epochs = 100\n\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Evaluation and Interpretation"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "To evaluate the model, we'll calculate the accuracy on the test set."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "with torch.no_grad():\n    X_test = torch.tensor(X_test, dtype=torch.float32)\n    y_test = torch.tensor(y_test, dtype=torch.long)\n    outputs = model(X_test)\n    _, predicted = torch.max(outputs.data, 1)\n    correct = (predicted == y_test).sum().item()\n    accuracy = correct / y_test.size(0)\n    print(f'Accuracy: {accuracy * 100:.2f}%')"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Practical Application"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "NLL loss is suitable for classification problems with multiple classes, such as text classification, image classification, and more. When using NLL loss, remember to apply the softmax activation function to the output layer of your model to produce probability distributions."
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Next Steps"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "Now that you have learned about Negative Log Likelihood (NLL) Loss in PyTorch, you can explore other loss functions, such as Cross-Entropy Loss, Binary Cross-Entropy Loss, and Mean Squared Error Loss. Understanding various loss functions will help you choose the right one for your specific problem and improve your model's performance."
        ],
        "metadata": {}
    }
],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
