{
   "cells": [
    {
        "cell_type": "markdown",
        "source": [
            "# Negative Log Likelihood (NLL) Loss\n",
            "\n",
            "In this tutorial, we'll explore the Negative Log Likelihood (NLL) Loss function in PyTorch. We'll cover the following key concepts:\n",
            "\n",
            "1. Explanation of NLL Loss\n",
            "2. Contextualizing the use of NLL Loss\n",
            "3. PyTorch implementation of NLL Loss\n",
            "4. Evaluating and interpreting results\n",
            "5. Practical applications\n",
            "6. Next steps\n"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Explanation of NLL Loss\n",
            "\n",
            "Negative Log Likelihood (NLL) Loss is a common loss function used in classification problems, especially when dealing with multi-class classification. It measures the dissimilarity between the predicted probabilities and the true class labels. Lower NLL Loss indicates that the model's predicted probabilities are closer to the true labels.\n",
            "\n",
            "In multi-class classification, we typically use the Softmax function to convert raw logits into probabilities. The NLL Loss then computes the negative log likelihood of the true class label given the predicted probabilities. The goal is to minimize this loss during training, leading the model to assign higher probabilities to the correct class."
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Contextualizing the use of NLL Loss\n",
            "\n",
            "Let's consider a multi-class classification problem where we want to categorize images of handwritten digits (0-9) using the popular MNIST dataset. We'll use PyTorch to create a simple neural network, and employ NLL Loss as our loss function.\n",
            "\n",
            "Note that we'll use the `torch.nn.LogSoftmax` activation function in the final layer of our model, which combines the Softmax function and the logarithm. This is a more numerically stable approach than using Softmax followed by NLL Loss separately."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.optim as optim\n",
            "from torchvision import datasets, transforms\n",
            "\n",
            "# Define the neural network\n",
            "class SimpleNet(nn.Module):\n",
            "    def __init__(self):\n",
            "        super(SimpleNet, self).__init__()\n",
            "        self.fc1 = nn.Linear(28 * 28, 128)\n",
            "        self.fc2 = nn.Linear(128, 64)\n",
            "        self.fc3 = nn.Linear(64, 10)\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = x.view(-1, 28 * 28)\n",
            "        x = torch.relu(self.fc1(x))\n",
            "        x = torch.relu(self.fc2(x))\n",
            "        x = self.fc3(x)\n",
            "        return nn.functional.log_softmax(x, dim=1)\n",
            "\n",
            "model = SimpleNet()\n",
            "print(model)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## PyTorch implementation of NLL Loss\n",
            "\n",
            "Now that we've defined our neural network, let's set up the NLL Loss function, optimizer, and data loaders for the MNIST dataset."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "# Set up NLL Loss, optimizer, and data loaders\n",
            "loss_function = nn.NLLLoss()\n",
            "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
            "\n",
            "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
            "train_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=True, download=True, transform=transform), batch_size=64, shuffle=True)\n", 
            "test_loader = torch.utils.data.DataLoader(datasets.MNIST('data', train=False, transform=transform), batch_size=1000, shuffle=False)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Training Process\n",
            "\n",
            "With everything set up, we'll train our model for a few epochs and observe the training loss."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "def train(model, device, train_loader, loss_function, optimizer, epoch):\n",
            "    model.train()\n",
            "    for batch_idx, (data, target) in enumerate(train_loader):\n",
            "        data, target = data.to(device), target.to(device)\n",
            "        optimizer.zero_grad()\n",
            "        output = model(data)\n",
            "        loss = loss_function(output, target)\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "        if batch_idx % 100 == 0:\n",
            "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n",
            "\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "model.to(device)\n",
            "\n",
            "for epoch in range(1, 6):\n",
            "    train(model, device, train_loader, loss_function, optimizer, epoch)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Evaluation and Interpretation\n",
            "\n",
            "Now that our model is trained, let's evaluate it on the test dataset and calculate the test loss and accuracy."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "def test(model, device, test_loader, loss_function):\n",
            "    model.eval()\n",
            "    test_loss = 0\n",
            "    correct = 0\n",
            "    with torch.no_grad():\n",
            "        for data, target in test_loader:\n",
            "            data, target = data.to(device), target.to(device)\n",
            "            output = model(data)\n",
            "            test_loss += loss_function(output, target).item()  # Sum up batch loss\n",
            "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
            "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
            "\n",
            "    test_loss /= len(test_loader.dataset)\n",
            "    accuracy = 100. * correct / len(test_loader.dataset)\n",
            "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(test_loss, correct, len(test_loader.dataset), accuracy))\n",      
            "\n",
            "test(model, device, test_loader, loss_function)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Practical Application\n",
            "\n",
            "NLL Loss is often used in multi-class classification problems where the output is a probability distribution over classes. Examples include:\n",    
            "\n",
            "- Handwriting recognition, as shown in this tutorial\n",
            "- Natural Language Processing tasks like text classification, part-of-speech tagging, or named entity recognition\n",
            "- Image classification in computer vision\n",
            "\n",
            "When using NLL Loss, it's important to ensure that the output layer of your model produces log probabilities, either by using `LogSoftmax` activation or by applying the logarithm after Softmax."
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Next Steps\n",
            "\n",
            "Now that you have a better understanding of the Negative Log Likelihood (NLL) Loss function in PyTorch, you can explore other loss functions that might better suit your specific problem domain. Some other common loss functions include:\n",
            "\n",
            "- Cross-Entropy Loss for binary or multi-class classification\n",
            "- Mean Squared Error (MSE) Loss for regression tasks\n",
            "- L1 Loss and Smooth L1 Loss for robust regression\n",
            "\n",
            "Additionally, you can experiment with different model architectures, optimizers, and hyperparameters to improve the performance of your model."     
        ],
        "metadata": {}
    }
],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
