{
   "cells": [
    {
        "cell_type": "markdown",
        "source": [
            "## Cross-Entropy Loss"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "Cross-Entropy Loss, also known as Log Loss, is commonly used in classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. It is defined as:"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "$$L(y,\\hat{y}) = -\\frac{1}{N}\\sum_{i=1}^{N}[y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$$"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "where $y$ is the true label (0 or 1), $\\hat{y}$ is the predicted probability, and $N$ is the number of samples."
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "In this tutorial, we will use a simple dataset to demonstrate how to apply Cross-Entropy Loss in PyTorch. We will create a binary classification problem where we need to classify whether a given number is even or odd."
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "## PyTorch Cross-Entropy Loss Parameters"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "In PyTorch, the `nn.CrossEntropyLoss` class is used for cross-entropy loss. It combines the `nn.LogSoftmax()` and `nn.NLLLoss()` (Negative Log Likelihood Loss) in one single class. The main parameters for this class are:"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "- `weight` (Tensor, optional): A manual rescaling weight given to each class. If provided, it has to be a Tensor of size `C`."
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "- `size_average` (bool, optional): Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when `reduce` is `False`. Default: `True`"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "- `ignore_index` (int, optional): Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets."
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "- `reduce` (bool, optional): Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "- `reduction` (string, optional): Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: No reduction will be applied, `'mean'`: The sum of the output will be divided by the number of elements in the output, `'sum'`: The output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "Now, let's import the necessary libraries and create a simple dataset for our binary classification problem."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "import torch\n",
            "import torch.nn as nn\n",
            "import numpy as np\n",
            "\n",
            "# Create a simple dataset\n",
            "data = torch.tensor([[2, 0], [5, 1], [8, 0], [12, 0], [15, 1], [17, 1]], dtype=torch.float32)\n",
            "X = data[:, 0]\n",
            "y = data[:, 1]\n",
            "\n",
            "print('Dataset:')\n",
            "print(data)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Import Libraries and Create Dataset"
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "import torch\n",
            "import torch.nn as nn\n",
            "\n",
            "# Create a simple dataset\n",
            "data = torch.tensor([[2, 0], [5, 1], [8, 0], [12, 0], [15, 1], [17, 1]], dtype=torch.float32)\n",
            "X = data[:, 0]\n",
            "y = data[:, 1]\n",
            "\n",
            "print('Dataset:')\n",
            "print(data)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Feedforward Neural Network"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "We will create a simple feedforward neural network with one input layer, one hidden layer, and one output layer. The hidden layer will have two neurons, and we will use the ReLU activation function. The output layer will have a single neuron with a sigmoid activation function to produce a probability value between 0 and 1."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "# Define the neural network model\n",
            "class SimpleNN(nn.Module):\n",
            "\n",
            "    def __init__(self):\n",
            "        super(SimpleNN, self).__init__()\n",
            "\n",
            "        # Define layers\n",
            "        self.hidden_layer = nn.Linear(1, 2)\n",
            "        self.output_layer = nn.Linear(2, 1)\n",
            "\n",
            "        # Define activation functions\n",
            "        self.relu = nn.ReLU()\n",
            "        self.sigmoid = nn.Sigmoid()\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.hidden_layer(x)\n",
            "        x = self.relu(x)\n",
            "        x = self.output_layer(x)\n",
            "        x = self.sigmoid(x)\n",
            "\n",
            "        return x\n",
            "\n",
            "# Instantiate the model\n",
            "model = SimpleNN()\n",
            "print(model)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Forward Propagation"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "Now that we have our neural network model defined, we can perform forward propagation by passing the input data through the model. This will produce an output probability value for each input data point."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "# Perform forward propagation\n",
            "X_input = X.view(-1, 1)  # Reshape input data\n",
            "\n",
            "# Pass the input data through the model\n",
            "output_probs = model(X_input)\n",
            "\n",
            "print('Output Probabilities:')\n"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Interpretation and Evaluation"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "The output probabilities can be interpreted as the model's prediction of whether a given number is even or odd. We can convert these probabilities to binary predictions by applying a threshold (e.g., 0.5). Note that the model has not been trained yet, so the predictions are likely to be incorrect at this stage."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "# Convert probabilities to binary predictions\n",
            "threshold = 0.5\n",
            "predictions = (output_probs > threshold).float()\n",
            "\n",
            "# Compare predictions with true labels\n",
            "print('Predictions:')\n",
            "print(predictions.view(-1))\n",
            "print('True labels:')\n",
            "print(y)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Practical Application"
        ],
        "metadata": {}
    },
    {
        "cell_type": "markdown",
        "source": [
            "In practical applications, forward propagation is an essential step in training and evaluating neural networks. Once the model is trained, you can use forward propagation to make predictions on new data and assess the performance of the model. For example, you could use a similar neural network to classify images, detect anomalies in time-series data, or predict stock prices."
        ],
        "metadata": {}
    }
],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
