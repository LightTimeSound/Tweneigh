{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### PPO Overview\n",
          "\n",
          "Proximal Policy Optimization (PPO) is a popular policy optimization algorithm for reinforcement learning, developed by OpenAI. It is an on-policy algorithm that strikes a balance between ease of implementation, sample efficiency, and ease of use. PPO is a general-purpose algorithm that can work well in various environments.\n",
          "\n",
          "The key idea behind PPO is to update the policy in a way that does not deviate too much from the previous policy. This is achieved by introducing a surrogate objective function with a clipped probability ratio. The clipping prevents overly large policy updates, which can lead to unstable training. PPO has shown strong performance across a variety of tasks, and it has been widely adopted in the reinforcement learning community."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Import necessary libraries\n",
          "\n",
          "First, we need to import the necessary libraries for our implementation. We'll be using the gym library to create the environment, PyTorch for deep learning, and stable_baselines3 for the PPO algorithm."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "import gym\n",
          "import torch\n",
          "from stable_baselines3 import PPO\n",
          "from stable_baselines3.common.vec_env import DummyVecEnv\n",
          "from stable_baselines3.common.evaluation import evaluate_policy"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Create an environment using gym\n",
          "\n",
          "We'll create a reinforcement learning environment using the gym library. In this example, we'll use the 'CartPole-v1' environment, a classic control problem. The objective is to balance a pole on a cart that moves along a track by applying forces to the cart. We wrap the environment in a DummyVecEnv to make it compatible with stable_baselines3, which expects vectorized environments by default."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "env_name = 'CartPole-v1'\n",
          "env = gym.make(env_name)\n",
          "env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVecEnv to use with stable_baselines3"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Initialize the PPO agent\n",
          "\n",
          "Now we'll initialize the PPO agent using the 'MlpPolicy' architecture, which is a feedforward neural network. We pass the environment and set the verbosity level to 1, so the training progress will be displayed during the learning process."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "model = PPO('MlpPolicy', env, verbose=1)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Train the PPO agent\n",
          "\n",
          "With the agent initialized, we can now train it using the learn() method. We set the total number of timesteps the agent will interact with the environment to 100,000. During the training process, the agent will learn a policy to balance the pole on the cart by applying forces."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "model.learn(total_timesteps=100000)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Evaluate the trained agent\n",
          "\n",
          "Finally, we'll evaluate the performance of the trained agent by running it in the environment for a few episodes. We use the evaluate_policy() function from stable_baselines3 to compute the mean and standard deviation of the rewards obtained during the evaluation. A higher mean reward indicates better performance."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
          "print(f'Mean reward: {mean_reward}, Std reward: {std_reward}')"
         ]
      }
   ]
   ,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
