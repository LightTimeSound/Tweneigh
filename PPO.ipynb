{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "## Proximal Policy Optimization (PPO)\n",
          "\n",
          "PPO is a popular Reinforcement Learning (RL) algorithm widely used for its simplicity and effectiveness in various environments. PPO is an on-policy algorithm that aims to strike a balance between policy improvement and policy stability. It is suitable for both continuous and discrete action spaces.\n",
          "\n",
          "In this tutorial, we will use the `CartPole-v1` environment from OpenAI Gym to demonstrate how PPO works with stable_baselines3 and PyTorch. The goal is to balance a pole on a cart by applying forces to the cart. We will cover the following sections:\n",
          "\n",
          "1. Setting up the environment\n",
          "2. Configuring PPO\n",
          "3. Training the agent\n",
          "4. Saving and loading the model\n",
          "5. Evaluating the agent\n",
          "6. Visualizing the agent's performance\n"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "# Required imports\n",
          "import gym\n",
          "from stable_baselines3 import PPO\n",
          "from stable_baselines3.common.vec_env import DummyVecEnv\n",
          "from stable_baselines3.common.evaluation import evaluate_policy"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### 1. Setting up the environment\n",
          "\n",
          "We will create a `CartPole-v1` environment using OpenAI Gym and wrap it in a `DummyVecEnv` to ensure compatibility with stable_baselines3."
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "env = gym.make('CartPole-v1')\n",
          "env = DummyVecEnv([lambda: env])"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### 2. Configuring PPO\n",
          "\n",
          "We will now configure the PPO algorithm. Some important hyperparameters are:\n",
          "\n",
          "- `learning_rate`: The learning rate for the optimizer.\n",
          "- `n_steps`: The number of steps to run for each environment per update.\n",
          "- `batch_size`: Minibatch size for each gradient update.\n",
          "- `n_epochs`: Number of times to iterate through the entire batch when calculating the gradient.\n",
          "- `gamma`: Discount factor for future rewards.\n",
          "- `gae_lambda`: Factor for trade-off between bias and variance for Generalized Advantage Estimator (GAE).\n",
          "- `clip_range`: Clipping parameter to limit the change in policy during an update.\n",
          "\n",
          "You can customize these hyperparameters based on your specific problem or use the default settings."
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "model = PPO(\n",
          "    'MlpPolicy', env, learning_rate=0.0003, n_steps=2048, batch_size=64, n_epochs=10,\n",
          "    gamma=0.99, gae_lambda=0.95, clip_range=0.2, verbose=1\n",
          ")"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### 3. Training the agent\n",
          "\n",
          "Train the agent by calling the `learn()` method and specifying the total number of training timesteps."
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "model.learn(total_timesteps=100000)"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### 4. Saving and loading the model\n",
          "\n",
          "Save the trained model using the `save()` method and load it using the `load()` method for future use."
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "model.save('ppo_cartpole')\n",
          "model = PPO.load('ppo_cartpole')"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### 5. Evaluating the agent\n",
          "\n",
          "Evaluate the performance of the trained agent using the `evaluate_policy()` function from stable_baselines3. It returns the mean reward and the standard deviation over multiple episodes."
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
          "print(f'Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}')"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### 6. Visualizing the agent's performance\n",
          "\n",
          "To visualize the agent's performance, you can render the environment using the `render()` method. Note that this might not work in all environments and platforms, such as Jupyter notebooks or non-GUI systems."
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "for i in range(3):\n",
          "    obs = env.reset()\n",
          "    done = False\n",
          "    while not done:\n",
          "        action, _ = model.predict(obs)\n",
          "        obs, _, done, _ = env.step(action)\n",
          "        env.render()\n",
          "env.close()"
         ]
        }
   ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
