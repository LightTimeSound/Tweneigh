{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### PPO Overview\n",
          "\n",
          "Proximal Policy Optimization (PPO) is a popular policy optimization algorithm for reinforcement learning, developed by OpenAI. It is an on-policy algorithm that strikes a balance between ease of implementation, sample efficiency, and ease of use. PPO is a general-purpose algorithm that can work well in various environments.\n",
          "\n",
          "The key idea behind PPO is to update the policy in a way that does not deviate too much from the previous policy. This is achieved by introducing a surrogate objective function with a clipped probability ratio. The clipping prevents overly large policy updates, which can lead to unstable training. PPO has shown strong performance across a variety of tasks, and it has been widely adopted in the reinforcement learning community."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Import necessary libraries"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "import gym\n",
          "import torch\n",
          "from stable_baselines3 import PPO\n",
          "from stable_baselines3.common.vec_env import DummyVecEnv\n",
          "from stable_baselines3.common.evaluation import evaluate_policy"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Create an environment using gym"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "env_name = 'CartPole-v1'\n",
          "env = gym.make(env_name)\n",
          "env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVecEnv to use with stable_baselines3"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Initialize the PPO agent"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "model = PPO('MlpPolicy', env, verbose=1)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Train the PPO agent"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "model.learn(total_timesteps=100000)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "### Evaluate the trained agent"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
          "print(f'Mean reward: {mean_reward}, Std reward: {std_reward}')"
         ]
      }
   ]
   ,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
