{
    "cells": [
     {
         "cell_type": "markdown",
         "source": [
             "# An Overview of Stable Baselines3\n",
             "\n",
             "In this tutorial, we will be exploring Stable Baselines3 (SB3), an open-source library for reinforcement learning in Python. SB3 is built on top of PyTorch and provides a collection of high-quality implementations of popular reinforcement learning algorithms. The library is designed to be easy to use, customizable, and efficient.\n",
             "\n",
             "We will start by installing the necessary dependencies, then explore the key concepts and components of SB3, and finally, work through an example using the library."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Installing Stable Baselines3 and its dependencies\n",
             "!pip install stable-baselines3"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Key Concepts\n",
             "\n",
             "Stable Baselines3 is built around the following key concepts:\n",
             "\n",
             "1. **Environments**: These are the problem settings for reinforcement learning, where an agent interacts with an environment to achieve a specific goal. SB3 uses OpenAI Gym environments by default, but you can also create custom environments.\n",
             "2. **Agents**: The learning algorithms are represented as agents in SB3. These agents learn to make decisions by interacting with environments.\n", 
             "3. **Policies**: A policy is a function that maps observations from the environment to actions that the agent should take. Policies in SB3 can be neural networks or simple functions.\n",
             "4. **Training Process**: The training process involves the agent interacting with the environment, collecting experiences, and updating its policy based on these experiences.\n",
             "5. **Evaluation**: After training, the agent's performance is evaluated by measuring its ability to achieve the desired goal.\n",
             "\n",
             "Now let's dive into an example using SB3."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Import necessary libraries\n",
             "import gym\n",
             "from stable_baselines3 import PPO"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Creating an Environment\n",
             "\n",
             "We will use the `CartPole-v1` environment from OpenAI Gym. The goal of the agent in this environment is to balance a pole on a cart by applying forces to the cart. The agent receives a reward of +1 for each time step the pole remains upright."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Create the environment\n",
             "env = gym.make('CartPole-v1')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Creating an Agent\n",
             "\n",
             "We will use the Proximal Policy Optimization (PPO) algorithm as our agent. PPO is an on-policy algorithm that has been shown to perform well on various tasks. We will use the default settings for PPO provided by SB3."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Create the agent\n",
             "agent = PPO('MlpPolicy', env, verbose=1)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Training the Agent\n",
             "\n",
             "We will now train the agent for 10,000 time steps using the `learn()` function. The training process involves the agent interacting with the environment, collecting experiences, and updating its policy based on these experiences."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Train the agent\n",
             "agent.learn(total_timesteps=10000)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Evaluating the Agent\n",
             "\n",
             "After training, we can evaluate the agent's performance by running it in the environment and measuring the total reward obtained. We will run the agent for 10 episodes and print the total reward for each episode."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Evaluate the agent\n",
             "for episode in range(10):\n",
             "    obs = env.reset()\n",
             "    done = False\n",
             "    episode_reward = 0\n",
             "    while not done:\n",
             "        action, _ = agent.predict(obs)\n",
             "        obs, reward, done, _ = env.step(action)\n",
             "        episode_reward += reward\n",
             "    print(f'Episode {episode + 1}: Reward = {episode_reward}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Practical Applications\n",
             "\n",
             "Stable Baselines3 can be used for various real-world applications such as robotics, natural language processing, and game playing. By creating custom environments and using different algorithms, you can apply reinforcement learning to solve complex problems and optimize decision-making processes.\n",        
             "\n",
             "## Next Steps\n",
             "\n",
             "Now that you have an understanding of Stable Baselines3, you can explore other reinforcement learning algorithms provided by the library, create custom environments, or experiment with different policy architectures. To further enhance your understanding of reinforcement learning, you may also want to study related topics such as deep reinforcement learning, multi-agent systems, and inverse reinforcement learning."
         ],
         "metadata": {}
     }
 ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.11.1"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }
