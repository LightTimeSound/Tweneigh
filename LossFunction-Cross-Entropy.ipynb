{
    "cells": [
     {
         "cell_type": "markdown",
         "source": [
             "## Cross-Entropy Loss"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "Cross-Entropy Loss, also known as Log Loss, is commonly used in classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. It is defined as:"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "$$L(y,\\hat{y}) = -\\frac{1}{N}\\sum_{i=1}^{N}[y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$$"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "where $y$ is the true label (0 or 1), $\\hat{y}$ is the predicted probability, and $N$ is the number of samples."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "In this tutorial, we will use a simple dataset to demonstrate how to apply Cross-Entropy Loss in PyTorch. We will create a binary classification problem where we need to classify whether a given number is even or odd."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "## PyTorch Cross-Entropy Loss Parameters"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "In PyTorch, the `nn.CrossEntropyLoss` class is used for cross-entropy loss. It combines the `nn.LogSoftmax()` and `nn.NLLLoss()` (Negative Log Likelihood Loss) in one single class. The main parameters for this class are:"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "- `weight` (Tensor, optional): A manual rescaling weight given to each class. If provided, it has to be a Tensor of size `C`."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "- `size_average` (bool, optional): Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when `reduce` is `False`. Default: `True`"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "- `ignore_index` (int, optional): Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets."   
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "- `reduce` (bool, optional): Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "- `reduction` (string, optional): Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: No reduction will be applied, `'mean'`: The sum of the output will be divided by the number of elements in the output, `'sum'`: The output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "Now, let's import the necessary libraries and create a simple dataset for our binary classification problem."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "import torch\n",
             "import torch.nn as nn\n",
             "import numpy as np\n",
             "\n",
             "# Create a simple dataset\n",
             "data = torch.tensor([[2, 0], [5, 1], [8, 0], [12, 0], [15, 1], [17, 1]], dtype=torch.float32)\n",
             "X = data[:, 0]\n",
             "y = data[:, 1]\n",
             "\n",
             "print('Dataset:')\n",
             "print(data)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Training Process"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "In this section, we will create a simple neural network model and train it using the Cross-Entropy Loss. We will use the dataset we created earlier to demonstrate the training process."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "First, let's define our simple neural network model:"
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "class SimpleNN(nn.Module):\n",
             "    def __init__(self):\n",
             "        super(SimpleNN, self).__init__()\n",
             "        self.linear = nn.Linear(1, 2)\n",
             "\n",
             "    def forward(self, x):\n",
             "        x = self.linear(x)\n",
             "        return x\n",
             "\n",
             "model = SimpleNN()\n",
             "print('Model:')\n",
             "print(model)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "Next, let's set up the Cross-Entropy Loss, the optimizer, and the training loop:"
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "criterion = nn.CrossEntropyLoss()\n",
             "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
             "num_epochs = 1000\n",
             "\n",
             "X = X.unsqueeze(1)  # Reshape the input\n",
             "\n",
             "for epoch in range(num_epochs):\n",
             "    optimizer.zero_grad()\n",
             "    outputs = model(X)\n",
             "    loss = criterion(outputs, y.long())\n",
             "    loss.backward()\n",
             "    optimizer.step()\n",
             "\n",
             "    if (epoch + 1) % 100 == 0:\n",
             "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Saving and Loading Outputs"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "After training our model, we may want to save it for future use or to share it with others. In this section, we will demonstrate how to save and load the trained model using PyTorch."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "### Saving the Model"
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "model_path = 'simple_nn.pth'\n",
             "torch.save(model.state_dict(), model_path)\n",
             "print(f'Model saved to {model_path}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### Loading the Model"
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "loaded_model = SimpleNN()\n",
             "loaded_model.load_state_dict(torch.load(model_path))\n",
             "loaded_model.eval()\n",
             "print(f'Model loaded from {model_path}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "Now that we have loaded the model, we can use it for making predictions."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "sample_input = torch.tensor([3.0]).unsqueeze(1)\n",
             "output = loaded_model(sample_input)\n",
             "prediction = torch.argmax(output, dim=1)\n",
             "print(f'Prediction for input {sample_input.item()}: {prediction.item()}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Evaluation and Interpretation"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "After training our model, we need to evaluate its performance on a validation or test dataset. In this section, we will demonstrate how to evaluate the model using accuracy as the performance metric."   
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "First, let's create a simple test dataset:"
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "test_data = torch.tensor([[4, 0], [6, 0], [9, 1], [11, 1], [14, 0], [16, 0], [19, 1]], dtype=torch.float32)\n",
             "X_test = test_data[:, 0].unsqueeze(1)\n",
             "y_test = test_data[:, 1]\n",
             "\n",
             "print('Test Dataset:')\n",
             "print(test_data)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "Now, let's evaluate the model on the test dataset and calculate the accuracy:"
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "with torch.no_grad():\n",
             "    test_outputs = loaded_model(X_test)\n",
             "    test_predictions = torch.argmax(test_outputs, dim=1)\n",
             "    accuracy = torch.sum(test_predictions == y_test).item() / len(y_test)\n",
             "\n",
             "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "In this example, we used accuracy as the performance metric. However, depending on the problem and dataset, other metrics such as precision, recall, F1-score, or ROC-AUC might be more appropriate. It's important to choose the right metric based on the specific problem and context."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Practical Applications"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "Cross-Entropy Loss is widely used in various practical applications, particularly in classification problems. Some examples include:"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "- Image classification: Training deep learning models like Convolutional Neural Networks (CNNs) to classify images into different categories (e.g., object recognition in images, handwritten digit recognition)."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "- Natural language processing: Training models like Recurrent Neural Networks (RNNs) or Transformers for text classification tasks (e.g., sentiment analysis, spam detection, document categorization)."   
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "- Recommender systems: Training models to predict user preferences and make recommendations based on those preferences (e.g., movie recommendations, product recommendations)."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "- Medical diagnosis: Training models to identify diseases or conditions based on medical data (e.g., diagnosing cancer from medical images, predicting heart disease from patient data)."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "In these applications, using Cross-Entropy Loss helps the model to output probabilities that closely match the true distribution of the target classes, resulting in better classification performance."   
         ],
         "metadata": {}
     }
 ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.11.1"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }
