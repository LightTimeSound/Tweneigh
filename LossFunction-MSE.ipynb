{
    "cells": [
     {
         "cell_type": "markdown",
         "source": [
             "### 1. Introduction to Mean Squared Error (MSE) Loss"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "Mean Squared Error (MSE) Loss is a popular loss function used in regression problems. It calculates the average of the squared differences between the actual and predicted values. The goal is to minimize this value to improve the model's performance."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "### 1.1 When to use MSE Loss"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "MSE Loss is commonly used for regression problems when the distribution of the target variable is continuous and not heavily skewed. If the target variable has a skewed distribution, consider using other loss functions like Mean Absolute Error (MAE) or Huber Loss."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "### 2. Contextualize the Topic"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "In this tutorial, we'll use a simple dataset with two continuous features and a continuous target variable. We'll implement a linear regression model using PyTorch and optimize it using the MSE Loss."   
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "### 3. Explain Parameters and Settings"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "We'll use the following parameters and settings in our example:\n",
             "1. `learning_rate`: The step size for the optimizer during training (e.g., 0.01).\n",
             "2. `num_epochs`: The number of times the model will go through the entire dataset during training (e.g., 100).\n",
             "3. `torch.manual_seed()`: Set the random seed for reproducibility."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "import torch\n",
             "import torch.nn as nn\n",
             "\n",
             "torch.manual_seed(42)  # Set random seed for reproducibility\n",
             "learning_rate = 0.01\n",
             "num_epochs = 100"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### 3.1 Effect of Outliers on MSE Loss"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "MSE Loss is sensitive to outliers, as it squares the differences between actual and predicted values. If your dataset has many outliers, consider using alternative loss functions like Mean Absolute Error (MAE) or Huber Loss."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "### 4. Training Process"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "First, let's create a synthetic dataset, define the linear regression model, and the MSE Loss function."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Create synthetic dataset\n",
             "X = torch.randn(100, 2)\n",
             "y = 2 * X[:, 0] + 3 * X[:, 1] + torch.randn(100)\n",
             "\n",
             "# Define linear regression model\n",
             "class LinearRegression(nn.Module):\n",
             "    def __init__(self, input_dim):\n",
             "        super(LinearRegression, self).__init__()\n",
             "        self.linear = nn.Linear(input_dim, 1)\n",
             "    \n",
             "    def forward(self, x):\n",
             "        return self.linear(x)\n",
             "\n",
             "model = LinearRegression(2)\n",
             "mse_loss = nn.MSELoss()\n",
             "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "Now, let's train the model using the following steps:\n",
             "1. Forward pass: compute the predictions using the model.\n",
             "2. Compute the loss using the MSE Loss function.\n",
             "3. Backward pass: compute the gradients of the loss with respect to the model parameters.\n",
             "4. Update the model parameters using the optimizer."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "for epoch in range(num_epochs):\n",
             "    # Forward pass\n",
             "    predictions = model(X)\n",
             "    \n",
             "    # Compute loss\n",
             "    loss = mse_loss(predictions, y.view(-1, 1))\n",
             "    \n",
             "    # Backward pass\n",
             "    loss.backward()\n",
             "    \n",
             "    # Update model parameters\n",
             "    optimizer.step()\n",
             "    \n",
             "    # Zero gradients for the next iteration\n",
             "    optimizer.zero_grad()\n",
             "    \n",
             "    if (epoch + 1) % 10 == 0:\n",
             "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### 4.1 Visualizing the Loss Curve"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "To visualize the loss curve, we'll store the loss values for each epoch and plot them using the matplotlib library."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "import matplotlib.pyplot as plt\n",
             "\n",
             "loss_values = []\n",
             "\n",
             "for epoch in range(num_epochs):\n",
             "    # (same code as before)\n",
             "    \n",
             "    loss_values.append(loss.item())\n",
             "    \n",
             "    # (same code as before)\n",
             "\n",
             "plt.plot(loss_values)\n",
             "plt.xlabel('Epoch')\n",
             "plt.ylabel('MSE Loss')\n",
             "plt.title('Loss Curve')\n",
             "plt.show()"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### 5. Saving and Loading Outputs"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "We can save the trained model and load it for future use."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "torch.save(model.state_dict(), 'linear_regression_model.pth')\n",
             "\n",
             "# Load the saved model\n",
             "loaded_model = LinearRegression(2)\n",
             "loaded_model.load_state_dict(torch.load('linear_regression_model.pth'))"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### 6. Evaluation and Interpretation"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "To evaluate the model, we can calculate the MSE Loss on a new dataset and compare the predicted values with the actual values."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Create a new synthetic dataset for evaluation\n",
             "X_test = torch.randn(20, 2)\n",
             "y_test = 2 * X_test[:, 0] + 3 * X_test[:, 1] + torch.randn(20)\n",
             "\n",
             "# Predict using the loaded model\n",
             "predictions_test = loaded_model(X_test)\n",
             "loss_test = mse_loss(predictions_test, y_test.view(-1, 1))\n",
             "\n",
             "print(f'MSE Loss on test dataset: {loss_test.item():.4f}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### 6.1 Comparing Optimizers and Learning Rates"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "Experiment with different optimizers (e.g., Stochastic Gradient Descent, Adam, RMSprop) and learning rates to observe their impact on the model's performance and the convergence of the loss function."   
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "### 7. Practical Application"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "The trained linear regression model can be used to predict continuous target variables given new input data. The smaller the MSE Loss, the better the model's predictions are."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "# Predict a single data point\n",
             "new_data_point = torch.tensor([1.5, -0.5])\n",
             "prediction = loaded_model(new_data_point)\n",
             "print(f'Prediction for the new data point: {prediction.item():.4f}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### 7.1 Limitations and Non-linear Models"
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "Linear regression models may not be suitable for capturing non-linear relationships in the data. If your dataset exhibits non-linear patterns, consider using more complex models like polynomial regression, decision trees, or neural networks."
         ],
         "metadata": {}
     }
 ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.11.1"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }
