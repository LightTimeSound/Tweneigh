{
    "cells": [
     {
         "cell_type": "markdown",
         "source": [
             "# Environments in Stable-Baselines3\n",
             "\n",
             "In this tutorial, we will discuss environments in Stable-Baselines3, a popular reinforcement learning library. We will explore how to create, use, and customize environments for training and evaluating reinforcement learning models."
         ],
         "metadata": {}
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Setup\n",
             "\n",
             "First, let's install stable-baselines3 and gym if you haven't already."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "!pip install stable-baselines3 gym"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Creating an Environment\n",
             "\n",
             "Stable-Baselines3 uses OpenAI's Gym library for environment creation. The `gym.make()` function allows us to create an environment instance by passing the environment's ID. Let's create an instance of the CartPole environment."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "import gym\n",
             "\n",
             "env = gym.make('CartPole-v1')\n",
             "print(env)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Interacting with the Environment\n",
             "\n",
             "We can interact with the environment using the `reset()` method to initialize the environment and get the initial observation, and `step()` method to perform an action and get the next observation, reward, and other information."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "observation = env.reset()\n",
             "print('Initial observation:', observation)\n",
             "\n",
             "action = env.action_space.sample()\n",
             "observation, reward, done, info = env.step(action)\n",
             "print('Next observation:', observation)\n",
             "print('Reward:', reward)\n",
             "print('Done:', done)\n",
             "print('Info:', info)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Customizing Environments\n",
             "\n",
             "To create a custom environment, you can subclass the `gym.Env` class and implement the required methods: `__init__`, `step`, `reset`, and `render`. Additionally, you need to define the action and observation spaces using Gym's `Space` classes, such as `Discrete`, `Box`, or `Tuple`."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "from gym import spaces\n",
             "\n",
             "class CustomEnvironment(gym.Env):\n",
             "    def __init__(self):\n",
             "        super(CustomEnvironment, self).__init__()\n",
             "        self.observation_space = spaces.Box(low=0, high=100, shape=(2,))\n",
             "        self.action_space = spaces.Discrete(3)\n",
             "    \n",
             "    def step(self, action):\n",
             "        # Implement your environment's step logic here\n",
             "        pass\n",
             "    \n",
             "    def reset(self):\n",
             "        # Implement your environment's reset logic here\n",
             "        pass\n",
             "    \n",
             "    def render(self, mode='human'):\n",
             "        # Implement your environment's rendering logic here\n",
             "        pass"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Training with Stable-Baselines3\n",
             "\n",
             "Once you have an environment, you can use it to train a reinforcement learning model using Stable-Baselines3. Here's an example using the PPO algorithm on the CartPole environment."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "from stable_baselines3 import PPO\n",
             "\n",
             "model = PPO('MlpPolicy', env, verbose=1)\n",
             "model.learn(total_timesteps=10000)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Evaluating the Trained Model\n",
             "\n",
             "After training the model, we can evaluate its performance by running it on the environment and calculating the total reward for each episode."      
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "num_episodes = 5\n",
             "\n",
             "for episode in range(1, num_episodes + 1):\n",
             "    obs = env.reset()\n",
             "    done = False\n",
             "    episode_reward = 0\n",
             "\n",
             "    while not done:\n",
             "        action, _states = model.predict(obs, deterministic=True)\n",
             "        obs, reward, done, info = env.step(action)\n",
             "        episode_reward += reward\n",
             "\n",
             "    print(f'Episode {episode}: Reward = {episode_reward}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Next Steps\n",
             "\n",
             "Now that you have learned how to create, use, and customize environments in Stable-Baselines3, you can further explore different algorithms, experiment with hyperparameters, and create more complex custom environments. Also, you can dive into the Stable-Baselines3 documentation for more advanced topics and best practices."
         ],
         "metadata": {}
     }
 ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.11.1"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }
