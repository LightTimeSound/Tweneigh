{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "## A2C (Advantage Actor-Critic) Algorithm\n\n",
           "The A2C algorithm is a synchronous, deterministic variant of the Asynchronous Advantage Actor-Critic (A3C) algorithm. It is an on-policy algorithm that combines the benefits of both policy-gradient and value-based methods. The algorithm uses two separate networks: an actor network for learning the policy and a critic network for estimating the action-value function (Q-value). The advantage function is used to determine how much better an action is compared to the average action at a given state.\n\n",
           "In this tutorial, we'll walk through the following steps:\n\n",
           "1. Install stable_baselines3 and gym\n",
           "2. Import necessary libraries\n",
           "3. Create the gym environment\n",
           "4. Initialize the A2C agent\n",
           "5. Train the agent\n",
           "6. Evaluate the trained agent\n"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "### Step 1: Install stable_baselines3 and gym\n\n",
           "Before we begin, we need to install the stable_baselines3 library, which provides the A2C implementation, and the gym library, which provides the environment for our agent to interact with."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "!pip install stable-baselines3[extra] gym"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "### Step 2: Import necessary libraries\n\n",
           "Now, let's import the required libraries for this tutorial."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "import gym\n",
           "from stable_baselines3 import A2C\n",
           "from stable_baselines3.common.evaluation import evaluate_policy"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "### Step 3: Create the gym environment\n\n",
           "We'll be using the CartPole-v0 environment from the gym library for this tutorial. This is a simple environment where the agent learns to balance a pole on a cart. Create the environment using the `gym.make()` function."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "env = gym.make('CartPole-v0')"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "### Step 4: Initialize the A2C agent\n\n",
           "To initialize the A2C agent, we'll create an instance of the `A2C` class from the stable_baselines3 library. We need to pass the policy architecture and the environment to the constructor. In this case, we'll use the default MLP policy architecture."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "agent = A2C('MlpPolicy', env, verbose=1)"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "### Step 5: Train the agent\n\n",
           "Now, we'll train the agent using the `learn()` method. This function takes the number of time steps to train the agent as an argument. In this example, we'll train the agent for 50,000 time steps."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "agent.learn(total_timesteps=50000)"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "### Step 6: Evaluate the trained agent\n\n",
           "Once the agent is trained, we can evaluate its performance using the `evaluate_policy()` function from the stable_baselines3 library. This function takes the trained agent, the environment, and the number of episodes to evaluate as arguments. It returns the mean and standard deviation of the rewards obtained during the evaluation."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "mean_reward, std_reward = evaluate_policy(agent, env, n_eval_episodes=10)\n",
           "print(f'Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}')"
         ]
       }
   ]
   ,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
