{
   "cells": [
    {
        "cell_type": "markdown",
        "source": [
            "# A2C Algorithm Tutorial\n",
            "\n",
            "In this tutorial, we will learn about the Advantage Actor-Critic (A2C) algorithm, an asynchronous variant of the Actor-Critic method. A2C is a popular reinforcement learning algorithm that combines the benefits of both policy gradient and value-based methods. It is designed to work well in continuous action spaces.\n",
            "\n",
            "We will use the `stable_baselines3` library, which provides an implementation of A2C, along with the `gym` library to set up an environment for training and testing the algorithm.\n",
            "\n",
            "## Key Concepts\n",
            "\n",
            "1. **Actor-Critic Method**: A2C is a type of Actor-Critic method, which maintains two separate neural networks: the *actor* network, responsible for selecting actions, and the *critic* network, responsible for estimating the value function.\n",
            "2. **Advantage Function**: The advantage function represents the difference between the value function and the expected value of a state-action pair. It measures how much better an action is compared to 
the average action in a given state.\n",
            "3. **Asynchronous Updates**: A2C uses parallel workers to collect experiences from different environments simultaneously, making it more sample-efficient than its synchronous counterpart, A3C.\n",       
            "\n",
            "## Environment Setup\n",
            "\n",
            "In this tutorial, we will use the `CartPole-v1` environment provided by the `gym` library. The agent's goal is to balance a pole on a cart. The agent can apply a force to the cart to move it left or right. The episode ends if the pole falls or the cart moves out of bounds. The agent receives a reward of +1 for every time step the pole remains balanced."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "import gym\n",
            "\n",
            "env = gym.make('CartPole-v1')"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## Installation\n",
            "\n",
            "Before we proceed, make sure you have `stable_baselines3` and `gym` installed. You can install them using the following command:"
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "!pip install stable-baselines3 gym"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "## A2C Implementation using stable_baselines3\n",
            "\n",
            "Now that we have set up our environment, let's implement the A2C algorithm using the stable_baselines3 library.\n",
            "\n",
            "### Importing necessary libraries"
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "from stable_baselines3 import A2C"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "### A2C Parameters and Settings\n",
            "\n",
            "Let's discuss some important hyperparameters and settings for the A2C algorithm:\n",
            "\n",
            "1. **`gamma`**: Discount factor for future rewards. A value between 0 and 1, with higher values placing more importance on long-term rewards.\n",
            "2. **`n_steps`**: The number of steps to run for each environment per update.\n",
            "3. **`lr_schedule`**: Learning rate schedule. It can be either `constant`, `linear`, or a custom schedule function.\n",
            "4. **`ent_coef`**: Entropy coefficient for the loss calculation. This controls the exploration-exploitation trade-off.\n",
            "5. **`vf_coef`**: Value function coefficient for the loss calculation. It determines the importance of the critic's loss compared to the actor's loss.\n",
            "\n",
            "Now, let's create an A2C model with some custom parameters."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "model = A2C('MlpPolicy', env, gamma=0.99, n_steps=5, lr_schedule='constant', ent_coef=0.01, vf_coef=0.5, verbose=1)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    },
    {
        "cell_type": "markdown",
        "source": [
            "### Training Process\n",
            "\n",
            "We can train the A2C model by calling the `learn()` method and passing the desired number of training timesteps. The method returns the trained model."
        ],
        "metadata": {}
    },
    {
        "cell_type": "code",
        "source": [
            "trained_model = model.learn(total_timesteps=100000)"
        ],
        "metadata": {},
        "outputs": [],
        "execution_count": null
    }
],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
