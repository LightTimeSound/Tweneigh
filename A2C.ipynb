{
    "cells": [
     {
         "cell_type": "markdown",
         "source": [
             "# A2C Algorithm Tutorial\n",
             "\n",
             "In this tutorial, we will learn about the Advantage Actor-Critic (A2C) algorithm, an asynchronous variant of the Actor-Critic method. A2C is a popular reinforcement learning algorithm that combines the benefits of both policy gradient and value-based methods. It is designed to work well in continuous action spaces.\n",
             "\n",
             "We will use the `stable_baselines3` library, which provides an implementation of A2C, along with the `gym` library to set up an environment for training and testing the algorithm.\n",
             "\n",
             "## Key Concepts\n",
             "\n",
             "1. **Actor-Critic Method**: A2C is a type of Actor-Critic method, which maintains two separate neural networks: the *actor* network, responsible for selecting actions, and the *critic* network, responsible for estimating the value function.\n",
             "2. **Advantage Function**: The advantage function represents the difference between the value function and the expected value of a state-action pair. It measures how much better an action is compared to the average action in a given state.\n",
             "3. **Asynchronous Updates**: A2C uses parallel workers to collect experiences from different environments simultaneously, making it more sample-efficient than its synchronous counterpart, A3C.\n",       
             "\n",
             "## Environment Setup\n",
             "\n",
             "In this tutorial, we will use the `CartPole-v1` environment provided by the `gym` library. The agent's goal is to balance a pole on a cart. The agent can apply a force to the cart to move it left or right. The episode ends if the pole falls or the cart moves out of bounds. The agent receives a reward of +1 for every time step the pole remains balanced."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "import gym\n",
             "\n",
             "env = gym.make('CartPole-v1')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## Installation\n",
             "\n",
             "Before we proceed, make sure you have `stable_baselines3` and `gym` installed. You can install them using the following command:"
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "!pip install stable-baselines3 gym"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "## A2C Implementation using stable_baselines3\n",
             "\n",
             "Now that we have set up our environment, let's implement the A2C algorithm using the stable_baselines3 library.\n",
             "\n",
             "### Importing necessary libraries"
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "from stable_baselines3 import A2C"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### A2C Parameters and Settings\n",
             "\n",
             "Let's discuss some important hyperparameters and settings for the A2C algorithm:\n",
             "\n",
             "1. **`gamma`**: Discount factor for future rewards. A value between 0 and 1, with higher values placing more importance on long-term rewards.\n",
             "2. **`n_steps`**: The number of steps to run for each environment per update.\n",
             "3. **`lr_schedule`**: Learning rate schedule. It can be either `constant`, `linear`, or a custom schedule function.\n",
             "4. **`ent_coef`**: Entropy coefficient for the loss calculation. This controls the exploration-exploitation trade-off.\n",
             "5. **`vf_coef`**: Value function coefficient for the loss calculation. It determines the importance of the critic's loss compared to the actor's loss.\n",
             "\n",
             "Now, let's create an A2C model with some custom parameters."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "model = A2C('MlpPolicy', env, gamma=0.99, n_steps=5, lr_schedule='constant', ent_coef=0.01, vf_coef=0.5, verbose=1)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### Training Process\n",
             "\n",
             "We can train the A2C model by calling the `learn()` method and passing the desired number of training timesteps. The method returns the trained model."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "trained_model = model.learn(total_timesteps=100000)"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### Saving and Loading Trained Models\n",
             "\n",
             "Once the training is complete, we can save the trained model to a file using the `save()` method. We can later load the saved model using the `load()` method."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "trained_model.save('a2c_cartpole')\n",
             "loaded_model = A2C.load('a2c_cartpole')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### Evaluation and Interpretation\n",
             "\n",
             "To evaluate the performance of our trained model, we can run it on the environment and observe the total rewards it accumulates over multiple episodes. We will also render the environment to visualize the agent's performance."
         ],
         "metadata": {}
     },
     {
         "cell_type": "code",
         "source": [
             "import numpy as np\n",
             "\n",
             "def evaluate(model, num_episodes=100):\n",
             "    env = gym.make('CartPole-v1')\n",
             "    rewards = []\n",
             "    for i in range(num_episodes):\n",
             "        obs = env.reset()\n",
             "        episode_reward = 0\n",
             "        while True:\n",
             "            action, _ = model.predict(obs, deterministic=True)\n",
             "            obs, reward, done, _ = env.step(action)\n",
             "            episode_reward += reward\n",
             "            if done:\n",
             "                break\n",
             "        rewards.append(episode_reward)\n",
             "    env.close()\n",
             "    return np.mean(rewards), np.std(rewards)\n",
             "\n",
             "mean_reward, std_reward = evaluate(loaded_model)\n",
             "print(f'Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}')"
         ],
         "metadata": {},
         "outputs": [],
         "execution_count": null
     },
     {
         "cell_type": "markdown",
         "source": [
             "### Practical Application\n",
             "\n",
             "The A2C algorithm can be applied to a wide range of reinforcement learning problems. It is particularly useful in continuous action spaces and complex environments, such as robotic control tasks, autonomous vehicle navigation, and game playing.\n",
             "\n",
             "To use the A2C algorithm in a different environment or problem, simply replace the environment creation step (`gym.make('CartPole-v1')`) with the desired environment and adjust the hyperparameters as needed. You can find a list of available environments in the [OpenAI Gym documentation](https://gym.openai.com/envs/#classic_control).\n",
             "\n",
             "Remember to consider the specific requirements of your problem, such as action space, state space, and reward structure, when selecting and configuring the A2C algorithm."
         ],
         "metadata": {}
     }
 ],
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.11.1"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 }
