{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "## Twin Delayed Deep Deterministic Policy Gradient (TD3)\n",
          "\n",
          "TD3 is an off-policy, model-free reinforcement learning algorithm for continuous control tasks. It is an extension of the Deep Deterministic Policy Gradient (DDPG) algorithm, which addresses the problem of overestimation bias in Q-value estimation. TD3 introduces three key improvements over DDPG:\n",
          "\n",
          "1. Clipped Double-Q Learning: TD3 uses two separate Q-networks and takes the minimum of their estimates to reduce overestimation bias.\n",
          "2. Delayed Policy Updates: TD3 updates the policy network less frequently than the Q-networks to reduce the impact of overestimation bias on the policy.\n",
          "3. Target Policy Smoothing: TD3 adds noise to the target action during the target network update to improve exploration.\n",
          "\n",
          "In this tutorial, we'll implement the TD3 algorithm using the stable_baselines3 library with a gym environment."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "## Installation\n",
          "\n",
          "Before we begin implementing the TD3 algorithm, we need to install the required libraries: stable_baselines3, gym, and PyTorch. You can install them using the following pip command:"
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "!pip install stable-baselines3[extra] gym pytorch"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "## Importing Libraries and Setting up the Environment\n",
          "\n",
          "Now that we have installed the required libraries, let's import them and set up the gym environment. In this tutorial, we'll use the 'Pendulum-v0' environment, a classic continuous control task where the goal is to balance a pendulum in the upright position."
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "import gym\n",
          "from stable_baselines3 import TD3\n",
          "from stable_baselines3.common.vec_env import DummyVecEnv\n",
          "\n",
          "# Create the gym environment\n",
          "env = gym.make('Pendulum-v0')\n",
          "\n",
          "# Vectorize the environment\n",
          "env = DummyVecEnv([lambda: env])"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "## Training the TD3 Agent\n",
          "\n",
          "Now that we have set up the environment, let's create and train the TD3 agent using the stable_baselines3 library. We'll train the agent for 50000 time steps and then evaluate its performance."
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "# Create the TD3 agent\n",
          "agent = TD3('MlpPolicy', env, verbose=1)\n",
          "\n",
          "# Train the agent for 50000 time steps\n",
          "agent.learn(total_timesteps=50000)"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "## Evaluating the Trained TD3 Agent\n",
          "\n",
          "After training the TD3 agent, let's evaluate its performance by running it in the 'Pendulum-v0' environment for 5 episodes. We'll also visualize the agent's performance."
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "import time\n",
          "\n",
          "def evaluate(agent, env, num_episodes=5):\n",
          "    for episode in range(num_episodes):\n",
          "        obs = env.reset()\n",
          "        done = False\n",
          "        episode_reward = 0\n",
          "        while not done:\n",
          "            action, _ = agent.predict(obs, deterministic=True)\n",
          "            obs, reward, done, info = env.step(action)\n",
          "            episode_reward += reward\n",
          "            env.render()\n",
          "            time.sleep(0.01)\n",
          "        print(f'Episode {episode + 1}: Reward = {episode_reward}')\n",
          "\n",
          "# Evaluate the trained TD3 agent\n",
          "evaluate(agent, env)"
         ]
        },
        {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
          "Don't forget to close the environment after evaluation."
         ]
        },
        {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
          "env.close()"
         ]
        }
   ]
   ,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
