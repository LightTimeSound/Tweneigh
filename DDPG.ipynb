{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "## Deep Deterministic Policy Gradient (DDPG) Algorithm\n",
           "\n",
           "DDPG is an algorithm that combines the ideas of Deep Q-Networks (DQN) and policy gradients. It's an off-policy, model-free, online RL algorithm that works with continuous action spaces. DDPG is an actor-critic method that uses two neural networks: one for the actor (policy) and another for the critic (Q-function).\n",
           "\n",
           "In this tutorial, we will implement the DDPG algorithm using the stable_baselines3 library and apply it to a continuous control task in the OpenAI Gym environment. We will also use PyTorch as the backend for stable_baselines3.\n",
           "\n",
           "### Step 1: Import Libraries\n",
           "\n",
           "First, let's import all the necessary libraries. If you don't have stable_baselines3, gym, or PyTorch installed, please install them using `pip install stable-baselines3[extra] gym torch`."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "import torch\n",
           "import gym\n",
           "from stable_baselines3 import DDPG\n",
           "from stable_baselines3.common.noise import NormalActionNoise\n",
           "from stable_baselines3.common.monitor import Monitor\n",
           "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
           "from stable_baselines3.common.env_util import make_vec_env"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "### Step 2: Create the OpenAI Gym Environment\n",
           "\n",
           "Now, let's create the OpenAI Gym environment for our DDPG agent. We will use the Pendulum-v0 environment, which is a continuous control task. The goal is to keep a pendulum upright using a single control input (torque)."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "env_id = 'Pendulum-v0'\n",
           "env = make_vec_env(env_id, n_envs=1)"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "### Step 3: Configure the DDPG Agent\n",
           "\n",
           "Before we create the DDPG agent, we need to configure the action noise, which is crucial for exploration in continuous action spaces. We will use NormalActionNoise, which adds Gaussian noise to the actions produced by the policy network. This noise helps the agent explore the action space more effectively."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "action_noise = NormalActionNoise(mean=0, sigma=0.1)"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "### Step 4: Create the DDPG Agent\n",
           "\n",
           "Now, let's create the DDPG agent using the gym environment and the action noise we configured earlier. We will also set the `tensorboard_log` parameter to log the training progress, which can be visualized using TensorBoard."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "agent = DDPG('MlpPolicy', env, action_noise=action_noise, verbose=1, tensorboard_log='./tensorboard_logs/')"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "### Step 5: Train the DDPG Agent\n",
           "\n",
           "Finally, let's train the DDPG agent using the `learn()` method. We will train the agent for 100,000 timesteps. You can adjust the number of timesteps depending on your computational resources."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "agent.learn(total_timesteps=100000)"
         ]
       },
       {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
           "### Step 6: Evaluate and Save the Trained Agent\n",
           "\n",
           "After training the DDPG agent, we can evaluate its performance by running the trained policy in the Pendulum-v0 environment. We can also save the trained agent using the `save()` method for future use."
         ]
       },
       {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
           "agent.save('ddpg_pendulum')\n",
           "\n",
           "# Test the trained agent\n",
           "obs = env.reset()\n",
           "for _ in range(1000):\n",
           "    action, _states = agent.predict(obs, deterministic=True)\n",
           "    obs, reward, done, info = env.step(action)\n",
           "    env.render()\n",
           "env.close()"
         ]
       }
   ]
   ,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
